\documentclass[12pt,a4paper]{report}
\usepackage[12pt]{extsizes}
\usepackage[english,russian]{babel}
\usepackage[utf8]{inputenc}
\addto\captionsrussian{\renewcommand{\contentsname}{Содержание}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry} % Меняем поля страницы
 \usepackage{float}
\usepackage[linesnumbered,boxed]{algorithm2e}
\graphicspath{{imgs/}}
\geometry{left=2cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1cm}% верхнее поле
\geometry{bottom=2cm}% нижнее поле
\renewcommand{\baselinestretch}{1.5}
\renewcommand{\theenumi}{\arabic{enumi}}
\renewcommand{\labelenumi}{\arabic{enumi}}
\renewcommand{\theenumii}{.\arabic{enumii}}
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}
\renewcommand{\theenumiii}{.\arabic{enumiii}}
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}
\author{Вадим Кокарев}
\title{Автоматическая интерпретация результатов коллаборативной фильрации}


\begin{document}
\begin{titlepage}
\newpage
\begin{center}
\vspace{1cm}
Санкт-Петербургский государственный Политехнический университет  \\*
(СпбГПУ Политех) \\*
Институт прикладной математики и информатики \\*
\hrulefill
\end{center}
 
\flushright{Кафедра Прикладная математика и информатика}

\vspace{8em}

\begin{center}
\Large наброски квалификационной работы
\end{center}

\vspace{2.5em}
 
\begin{center}
\textsc{\textbf{Автоматическая интерпретация результатов работы рекомендательной системы}}
\end{center}

\vspace{6em}
 
\begin{flushleft}
Выполнил \hrulefill Кокарев В.В. \\
\vspace{1.5em}
\end{flushleft}
 
\vspace{\fill}

\begin{center}
Спб. 2015
\end{center}

\end{titlepage}
\tableofcontents % это оглавление, которое генерируется автоматически
\pagebreak
\chapter{Введение}
В последние годы, четко обозначилась тенденция к росту популярности интернет магазинов. Одним из основных отличий виртуального магазина от реального - ассортимент предлагаемых товаров. В силу физических причин, невозможно представить весь ассортимент товаров в обычном магазине, в отличии от виртуального магазина. С одной стороны не ограниченный ассортимент - безусловный плюс виртуальных магазинов, с другой он требует от покупателя потратить больше времени на поиск и выбор необходимого товара.

Обычный покупатель, в свою очередь, хочет тратить как можно меньше времени на поиск и выбор товара. С точки зрения покупателя, идеальным был бы сценарий, когда первая же ссылка на главной странице интернет-магазина, приводила бы его на товар, который он бы хотел купить в данный момент.

Данные рассуждения приводят к мысли, что простого поиска по по каталогу и системы фильров может быть не достаточно для успешного функционирования виртуального магазина. Возможно, не стоит дожидаться, пока пользователь потратит свое время на заполнение форм поиска, а попробовать угадать какой товар интересует пользователя в данную минуту и предложить его. Именно такую задачу и решают рекомендательные системы.

Рекомендательные системы, в первую очередь, предназначены для улучшения жизни конечных пользователей: экономии времени, подбора объективно хорошего товара. С точки зрения продавца, хороша рекомендательная система позволяет продать больше товаров, а так же повысить лояльность пользователей, оценивших скорость покупок в данном магазине.

\section{Способы улучшения качества рекомендаций}
Т.к. от хорошей рекомендательной системы выигрывают все (и покупатель, и продавец), очевидным желанием является улучшения качества рекомендаций.
Метрики, позволяющие оценить качество рекомендаций будут рассмотрены позже, пока, под качеством рекомендаций можно считать $P=\frac{buy}{show}$, где $buy$ - число покупок товаров по рекомендации, $show$ - число показов блока с рекомендациями.

Интуитивно можно выделить несколько направлений, возможно, приводящих к улучшению качества рекомендаций:
\begin{itemize}
\item увеличения объема данных, на которых собирается статистика (далее обучающее множество)
\item улучшение алгоритма формирования рекомендаций
\end{itemize}
Практика показывает, что в большинстве случаев, увеличение объема обычающего множества, действительно, позволяет улучшить качество рекомендаций. Однако, зависимость между мощностью обучающего множества и качеством рекомендаций, как правило, имеет логарифмический характер. Это приводит к тому, что хоть сколько-нибудь значимое изменение качества требует многократного изменения мощности обучающего множества. На практике такое практически невозможно.


Улучшение старого или разработка нового алгоритма формирования рекомендаций - процесс, требующий значительных затрат на исследования. А гарантий, что исследования дадут реальный результат - нет.

В итоге, каждое последующее улучшение качества рекомендаций требует больших ресурсов, но дает меньший результат.
Это приводит к мысли, что  необходимо искать альтернативные спобы увеличения числа покупок при том же качестве рекомендаций (при фиксированном алгоритме формирования рекомендаций и фиксированном обучающем множестве).

\section{Неформальная постановка задачи}
Психологи доказали, что люди намного охотнее принимают советы, содержащие обоснование. Т.е. если попробовать объяснять пользователю, почему рекомендательная система считает, что этот товар ему подходит, то пользователь будет позитивнее воспринимать предложенный товар. Так же прользователь может оценить тезис, представленный в рекомендации и в случае, если тезис, с его точки зрения ложный - не тратить время на просмотр товара, который он не купит.

Например, рекомендательная система предлагает купить бензопилу "Лесоруб", обосновывая это тем, что пользователь строит дачный дом. Если пользователь действительно строит дом и у него нет пилы, то он может задуматься, о покупке пилы, если даже не думал об этом ранее. Если пользователь не строит дом или у него есть пила, то он сразу поймет, что данная рекомендация ему не интересна и не будет тратить время на просмотр бесполезного товара.
Теперь рассмотрим того же пользователя и ту же рекомендацию пилы, но предположим, что обоснование рекомендации звучит так: "Данная пила лучше, чем "Лесоруб-0".
Ситуация получается диаметрально противоположной той, что была описана в предыдущем примере. Если у пользователя нет пилы, то он скорее всего пройдет мимо, т.к. информация о том, что какая-то пила лучше какой-то другой пилы для него не несет полезной информации. А если же, у пользователя есть пила "Лесоруб-0", то он может заинтересоваться более новой моделью.
Данные примеры показыавют, что один и тот же товар, может быть расценен по разному одним и тем же пользователем, в зависимости от представленного обоснования. Т.е. можно предполагать, что правильное объяснение рекомендаций, действительно может улучшить общее качество рекомендательной системы.

Целью данной работы является разработка алгоритма, способного найти наиболее вероятную причину, по которой алгоритм формироавния рекомендаций выбрал конкретный товар для конкретного пользователя.

\section{Примеры успешных продуктов}
Приведем несколько примеров, как коммерческих, так и некоммерческих рекомендательных систем.
Популярныый сайт про кино kinopoisk.ru (Рис. $\ref{ris:ex})$ предлагает к просмотру фильмы, обосновывая свои рекомендации похожестью на конкретный фильм.
Другой сайт imhonet.ru предлагает к просмотру фильм, ссылаясь на то, что он нравится каким-то людям, похожим по какому-то критерию на текущего пользователя.
\begin{figure}[H]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.8\linewidth]{imhonet} \\ imhonet.ru}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.9\linewidth]{kinopoisk} \\ kinopoisk.ru}
\end{minipage}
\caption{Примеры рекомендаций на различных сайтах}
\label{ris:ex}
\end{figure}

Из данных примеров видно, что даже очень крупные и популярные сайты не пренебрегают объяснениями причин рекомендаций. Однако, все объяснения являются статичными и не зависят от того, какой товар какому пользователю предлагается.

\chapter{Обзор рекомендательных систем}
Невозможно построить алгоритм формирования объяснения для рекомендаций, полностью абстрагировавшись от самого алгоритма формирования рекомендаций. В данной главе будут рассмотрены основные принципы, заложенные в различные рекомендательные системы, зная которые, можно будет переходить непосредственно к разработке алгоритма объяснения рекомендаций.
Методы подбора параметров (обучния модели) останутся за рамками изложения, т.к. не влияют на дальнейшее изложение.
\section{Классические рекомендательные системы}
Одними из наиболее ранних подходов к формированию рекомендаций стали подходы основанные на пользователе или продукте. Основная идея систем, сонованных на пользователе, заключается в том, что похожим пользователям нравятся похожие товары.
Т.е. что бы воспользоваться этим подходом необходимо ввести некоторую функцию похожести двух пользователей $sim: \mathfrak{R}^n \times \mathfrak{R}^n \to [0,1]$. Обычно, в качестве функции $sim$ выбирают коэффициент корреляции Пирсона. Оценка для конкретного товара получается как взвешенная сумма всех известных оценок, в качестве веса используется значение функции похожести. Стоит отметить, что можно добиться заметного улучшения качества рекомендаций, если предсказывать не абсолютное значение рейтинга, а его отклонение от среднего рейтинга для пользователя. Т.е. 
\begin{equation*}
\tilde{r_{ia}} = \bar{r_i} + \frac{\sum_{j} sim(u_i, u_j) * (r_{ja} - \bar{r_j})}{\sum_{j} sim(u_i, u_j)}
\end{equation*}
где $\tilde{r_{ia}}$ - предсказанный рейтинг для пользователя $i$ и товара $a$, $\bar{r_i}$ - средний рейтинг для пользователя $i$, $r_{ia}$ - известный рейтинг товара $a$ для пользователя $j$, $\bar{r_j}$ - средний рейтинг для пользователя $j$.

Заметим, что учитывать надо только тех пользователей, для которых заведомо известны оценки текущего товара.

Рекомендательные системы, основанные на продукте строятся аналогичным образом, но опираются на другую гипотезу: одному и тому же человеку нравятся похожие товары.

Так же иногда используют рекомендательные системы, основанные на знаниях. В таких рекомендательных системах заранее задается набор правил, которые полностью описывают процесс формирования рекомендаций. Примерами таких правил могут быть: "не рекомендовать боевики женщинам старше 38", "не рекомендовать фильмы с оценкой ниже 6" и т.д.

С точки зрения данной работы, рекомендательные системы, описанных типов, не представляют интереса, т.к. в них уже заложено решение поставленной задачи. Поэтому они рассматриваться не будут.

\subsection{Матричная факторизация}
Данный метод обрел свою популярность после успеха на Netflix Cup (соревнование по построению рекомендательной системы фильмов для сайта netflix.com). В данном соревновании необходимо было угадать какой рейтинг (от 1 до 5) поставит конкретный человек конкретному фильму, зная все оценки, которые кога-либо были поставлены на сайте.


Пусть $\{\mathcal{U}\}_{i=0}^{UserCount}$ - $UserCount$ - индексированное множество пользователей, когда-либо поставивших оценку, $\{\mathcal{I}\}_{i=0}^{ItemCount}$ - $ItemCount$ - индексированное множество товаром, имеющих хотя бы одну оценку. Заранее имеется набор троек $\mathcal{L} = \{u, a, r\}$, где $u \in \mathcal{U}, a \in \mathcal{I}, r \in [0, 5]$. Необходимо для каждой пары $\{u,a\} \notin \mathcal{L|_{\mathcal{U}\times\mathcal{I}}}$ найти значение рейтинга $r$. При этом необходимо минимизировать некоторый функционал качества предсказания $T$. В качестве функционала качества может выступать, например, RMSE.

Пусть $r_{ij}$ - рейтинг, который поставиль пользователь $u_i \in \mathcal{U}$ фильму $a_j \in \mathcal{I}$. Основная идея метода состоит в том, что $r_{ij}$ можно представить следующим образом:
\begin{equation*}
r_{ij} = \mu + bias_{u_i} + bias_{a_j} + q_{a_j}^Tp_{u_i}
\end{equation*}
где $\mu \in \mathbb{R}, bias \in \mathbb{R}, q,p \in \mathbb{R}^k, k - $ параметр модели.
Все величины подбираются методами численной оптимиации, но их можно интерпретировать следующи образом:
\begin{itemize}
\item $\mu$ - общее смещение модели, средний рейтинг всех товаров среди всех пользователей
\item $bias_u, bias_m$ - среднее смещений рейтинга, выставленного данным пользователем (или данному товару) относительно $\mu$
\item $p, q$ - вектора, каким-то образом описывающие конкретного пользователя/товар.
\end{itemize}

Метод получил свое название, благодаря оригинальному методу обучения. Исходные тройки представляются в виде матрицы: каждому пользователю соответствует строка, каждому товару - столбец, а на пересечении строки и столбца стоит рейтинг, если он известен. Далее профодится разложение данной матрицы на две компоненты $P$ и $Q$, которые состоят из векторов $p$ и $q$ соответственно.

\section{Рекомендательные системы, основанные на решении задачи классификации}
Заметим, что представленные методы не позволяют в полной мере использовать всю информацию, которая известна про пользователя или товар. Например, при регистрации пользователь может указать пол, возраст, родной город и многое другое.

Рассмотрим задачу бинарной классификации:

$\textit{Дано:}$
\begin{itemize}
\item $\mathbb{U}$ - множество пользователей;
\item $\mathbb{I}$ - множество товаров;
\item $\mathbb{Y} = \{-1,+1\}$ - множество меток классов;
\item $\mathbb{L} \subset \mathbb{U} \times \mathbb{I} \times \mathbb{Y}$ - множество примеров (прецедентов);
\item $\mathbb{T} \subset \mathbb{U} \times \mathbb{I} \times \mathbb{Y}$ - множество тестовых примеров (прецедентов), заметим, что $\mathbb{T|_{\mathbb{U} \times \mathbb{I}}} \cap \mathbb{L|_{\mathbb{U} \times \mathbb{I}}} = \emptyset$;
\end{itemize}
$\textit{Задача:}$ построить алгоритм $A: \mathbb{U} \times \mathbb{I} \to [0,1]$ используя только преценденты $e \in \mathbb{L} $ такой, что $Target(\mathbb{T}, A) \to \min$, где $Target$ - некоторый функционал качества (далее будем называть его функцией потерь). Алгоритм $A$ возвращает вероятность того, что пара $(u, a) \in  \mathbb{U} \times \mathbb{I}$ имеет метку $+1 \in \mathbb{Y}$.

Предположим, что имеется способ апостериори понять, понравился пользователю товар или нет (в простейшем случае можно считать, что покупка товара означает, что он понравился, отказ от покупки - противное), тогда можно посроить множества $\mathbb{T}$ и $\mathbb{L}$, присваивая метку $+1 \in \mathbb{Y}$ той паре пользователь-товар, где товар понравился пользователю и метку $-1 \in \mathbb{Y}$, где товар не понравился. Пусть алгоритм $A$ - решение данной задачи бинарной классификации для некоторой функции потерь $Target$. Тогда можно построить рекомендательную системы, работающую по следующему алгоритму:


\begin{algorithm}[H]
\SetAlgoLined
\KwData{$A$ - алгоритм классификации, $u \in \mathbb{U}$ - пользователь, $\mathbb{I}$ - множество товаров, $n \in \mathbb{N}$ - желаемое число рекомендаций}
\KwResult{$recomendationList$ - список пар из товаров и вероятности того, что они понравятся пользователю, $size(recomendationList) \le n$}
$ratings = SortedSet()$

\For{$a \in \mathbb{I}$} {
	$ratings \leftarrow (A(u, a), a)$ -- добавляем пару товар-вероятность в упопядоченную коллекцию, сортировка производится по убыванию веротяности
}

\textbf{return} $head(ratings, n)$ -- функция $head$ - извлекает $n$ товаров с наибольшей веротяностью
\caption{Использование классификатора для построения рекомендательной системы.}
\label{alg:CARS}
\end{algorithm}

Для удобства далее под алгоритмом формирования рекомендаций может пониматься не сам алгоритм, а алгоритм классификации, лежащий в его основе. Пусть $A$ - алгоритм построения рекомендаций, испольщующий алгоритм классификации $C$. Для краткости, условимся под обозначением $A(u, a)$, где $u \in \mathbb{U}, a \in \mathbb{I}$ понимать $C(u, a)$.

[TODO НАДО ЛИ ВООБЩЕ ГОВОРИТЬ ПРО ЗАДАЧУ РЕГРЕССИИ И ЛОГИТ-РЕГРЕСИИЮ? ЕСЛИ ДА, ТО КАК ВВЕСТИ?]

\subsection{Формальное описнаие объектов}
\newtheorem{Def}{Определение}
\begin{Def}
Признак --- $f: \mathbb{D}  \to \mathbb{R}$. Отображение сопоставляющее произвольному объекту $e \in \mathbb{D}$, принадлежащему некоторой предметной области $\mathbb{D}$ вещественное число. Для удобаства, под признаком часто понимают не само отображение, а его значение для конкретных параметров.
\end{Def}

Пусть дан набор признаков $\{f_i\}_{i=0}^n$, определенных для одной предметной области.
\begin{Def}
Признаковое описание объекта --- $x = Concat(f_0(e), \dots f_n(e)), e \in \mathbb{D}$, где $Concat$ - операция конкатенации произвольного числа векторов.
\end{Def}
Далее будем отождествлять объект с его описанием. Т.е. предполагается, что набор признаков способен однозначно определить объект в рамках решаемой задачи.

Приведем пример, пусть $\mathbb{D}$ - множество студентов, тогда в качестве признака могут выступать следующие отображения: средний бал за время обучения, номер курса, номер зачетной книжки, доля пропущенных занятий и др.

\subsection{Factorization machines}
На протяжении долгого времени, одним из самых популярных алгоритмов машинного обучения был метод опорных векторов (SVM) , однако, SVM не нашел широкого применения в задачах коллаборативной фильтрации, т.к. не способен построить надежную нелинейную разделяющую поверхность, опираясь на сильно разреженные данные. В задачах данного типа хорошо зарекомендовали себя методы, основывающиеся на идее матричной/тензорной факторизации, например, PARAFAC [можно вставить ссылку на Хершмана]. С другой стороны, модели на основе матричной факторизации сильно ограничивают тип входных данных и не могут быть применены к стандартным векторам признаков. Т.е. данные модели не позволяют использовать все знания про объекты предметной области, которыми мы обладаем.

Модель FM является SVM с полиномиальным ядром (см. kernel trick) с одним важным отличием: коэффициенты представляются в факторизованном виде (т.е. в виде пары векторов $v, u$, таких что $<u,v> = a \in \mathcal{R}$).

Модельное уравнение для FM 2-го порядка (модель более высоких порядков не имеют практической ценности):
\begin{equation}
\tilde{y}(x) = w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n}\sum_{j=i+1}^{n}<v_i, v_j>x_ix_j,
\end{equation}
где $w_0 \in \mathfrak{R}, w \in \mathfrak{R}^n, V \in \mathfrak{R}^{n \times k}$ - парметры модели, которые необходимо подобрать.
\begin{itemize}
\item $w_0$ --- общее смещение модели (вероятность, что случайное приложение понравится случайному пользователю)
\item $w_i$ --- вес $i$-й компоненты вектора признаков $x$ в данной модели
\item $v_i$ --- вещественный вектор размера $k$, описывающий $i$-й компонент вектора признаков. $k$ -  входной параметр алгоритма, описывающий глубину факторизации
\item $\tilde{w}_{i,j} = <v_i, v_j>$ --- вес взаимодействия $i$-й и $j$-й компоненты вектора признаков. Основной особенностью FM является то, что вместо прямого использования $\tilde{w}_{i,j}$ как параметра модели, используется их факторизованное представление. Именно это позволяет методу работать с сильно разреженными данными.
\end{itemize}

Подбор параметров модели ($w_0, w, V$) осуществляется с помощью различных алгоритмов стохастической оптимизации, например, SGD, ASGD, ALS.

Заметим, что в общем случае, Factorization Machines решает задачу восстановления регресии, но если в функции потерь использовать логистическую регрессию, то будет решена задача классификации. Кроме того, с помощью функции $f(x) = \frac{1}{1 + e^{-x}}$ оценить вероятность принадлежности объекта к тому или иному классу.

\subsection{Деревья принятия решений}
Дерево принятия решений - это бинарное дерево, у которого в узлах (не являющихся листами) находятся предикаты, связанные с признаками, а в листьях - значение целевой функции (например, для задачи бинарной классификации - вероятность принадлежности выделенному классу), а каждое ребро ассоциированно с конкретным значением предиката, находящегося в родительском узле.
Таким образом, дерево принятия решений является кусочно-постоянной функцией. т.к. разбивает пространство признаков на некоторое конечное число областей, внутри которых значение целевой функции считается константным. На Рис. $\ref{ris:dt}$ приведен пример дерева принятия решений и области, в пространстве признаков, где целевая функция считается постоянной.

\begin{figure}[H]
\center{\includegraphics[width=0.8\linewidth]{dt}}
\caption{Пример дерева принятия решений}
\label{ris:dt}
\end{figure}

\subsubsection{Забывчивые деревья принятия решений}
Рассмотрим подмножество деревьев принятия решений, называемое забывчивыми деревьями принятия решений.
Основным отличием забывчивых деревьев является то, что на каждом уровне дерева успользуется одно и тоже условие. 

\begin{figure}[H]
\center{\includegraphics[width=0.8\linewidth]{ot}}
\caption{Пример забывчивого дерева принятия решений}
\label{ris:ot}
\end{figure}

Безусловно это ограничение делает данный вид деревьев менее вариативными, что должно негативно влиять на качество классификации. Однако такая структура дерева позволяет эффективно хранить обученные деревья, а так же быстро вычислять значения целевой функции.

Закодируем путь из корня до каждого листа следующим образом: каждый переход по ребру, ассоциированному с истинностью условия будем обозначать за 1, а переход по ложному ребру за 0. Т.о. путь до каждого листа можно закодировать строкой длины $(d-1)$, состоящей из 0 и 1, где $d$ -- глубина дерева. Будем интерпретировать строку, сопоставленную каждому листу, как запись некоторого двоичного числа, тогда каждому листу будет сопоставлено натуральное число от 1 до $2^{d-1}$.
Таким образом значения каждого листа можно записать в массив длинной $2^{d-1}$ на позицию, ассоциированную с данным листом. Если так же записать массив из $(d-1)$ бинарных предикатов, то этого будет достаточно, что бы однозначно восстановить дерево.
Т.е. каждое забывчивое дерево можно формально представить в виде пары $(\{Condition\}_{i=0}^{в - 1}, \{Score\}_{i=0}^{2^{d - 1} - 1})$, где $\{Condition\}_{i=0}^{d - 2}$ - индексированное множество условий, $\{Score\}_{i=1}^{2^{d - 1}}$ - индексированное множество значений целеой функции.

Вычисление значений целевой функции для забывчивых деревьев крайне эффективно, достаточно составить бинарное число, у которого на месте $i$-го разряда будет стоять значение предиката $Condition_i$ и вернуть элемент массива $Score$ с данным индексом.

Благодаря эффективности вычисления значения целевой функции данный вид деревьев широко применяется на практике в составе ансамблей решений, которые будут рассмотрены далее.
\subsection{Ансамбли решений}
Пусть $\mathbb{H}$ - некоторое семейство базовых алгоритмов классификации (или восстановления регрессии), каждый элемент $h(x; a) \in \mathbb{H} : X \to \mathbb{R}$ определяется некоторым вектором параметров $a \in \mathbb{A}$.

Предположим, что дано $m$ (для простоты, $m$ - не четное) независимых классификаторов $\{h(x, a_i)\}_{i=1}^m$ и для каждого классификатора вероятность ошибиться составляет $p < 0.5$.	Построим классификатор, опрашивающий все $m$ базовых классификатора и возвращающий ответ, принятый большинством. Тогда вероятность ошибки для такого классификатора составляет: $P = \sum_{i = m/2 + 1}^m \binom{m}{i}p^{i}(1-p)^{m-i}$. Например, при $m=25$ и $p=0.35$ $P=0.06$.

К сожалению, на практике такие результаты недостижимы, т.к. обычно результаты работы различных классификаторов сильно коррелированы, но идея комбинирования нескольких "плохих" классификаторов в один "хорший" крайне популярна.

\begin{Def}
Алгоритм $F(x) = \sum_{i=0}^{M}b_ih(x,a_i), b_m \in \mathbb{R}, a_i \in \mathbb{A}$ будем назвать ансамблем решений.
\end{Def}
При правильном подборе параметров ансамбли решений способны показывать результаты, многократно превосходящие результаты любого из базовых алгоритмов.


Базовые алгоритмы принято называть $\textit{слабыми решениями}$, а сам ансамбль - $\textit{сильным решением}$.
Заметим, что в качестве слабых решений можно использовать различные алгоритмы одновременно , например, SVM и деревья решений. Однако, на практике такой подход применяется крайне редко.

В данной работе, в качестве конкретной реализации ансамблей решений, будут рассмотрены ансамбли забывчивых деревьев решений, т.к. они наилучшим образом проявили себя при построении рассматриваемой рекомендательной системы.

\chapter{Задача формирования рекомендаций мобильных приложений}
Не умаляя общности, в данной работе будет рассматриваться задача построения рекомендательной системы для магазина мобильных приложения под платформу Android. Однако, предложенные в работе принципы могут быть применены к произвольной предметной области.
\section{Исходные данные}
Для магазина мобильных приложений все исходные данные можно разделить на 3 типа:
\begin{itemize}
\item данные про пользователей
\item данные про приложения
\item данные про пару пользователь - приложение (статистика использования конкретного приложения конкретным пользователем).
\end{itemize}
Данные про приложения можно получить из магазина приложений Google Play. К таким заниям относятся:
\begin{itemize}
\item категория (игры, навигация, etc)
\item текстовое описание
\item комментарии пользователей
\item средняя оценка пользователей
\item кол-во установок
\item версии операционной системы, для которых доступно данное приложение
\item etc.
\end{itemize}
Так же можно извлечь данные из анонимной статистики, собираемой на устройствах пользователя:
\begin{itemize}
\item среднее время от открытия до закрытия приложения
\item распределение числа запусков по времени суток
\item кол-во установок и удалений за период времени
\item среднее время от установки на устройство пользователя до удаления
\item etc
\end{itemize}
Данные про пользователя можно извлечь как из статистики с устройства пользователя, так и анализируя поведение пользователя в магазине (в приложении или на веб-интерфейсе):
\begin{itemize}
\item список установленных приложений
\item установки/запуски/удаления приложений с привязкой ко времени и координатам
\item факт просмотра конкретного приложения в магазине
\item модель телефона
\item версия ОС
\item etc.
\end{itemize}
На основе перечисленных данных строится набор признаков, с помощью которого вычисляется признаковое описание для пары пользователь-приложение. Напомню, что признаковое описание объекта и сам объект отождествляются.

\section{Оценка качества рекомендательной системы}
В качетсве показателей качества рекомендательной системы мобильных приолжений можно выбрать разные величины. Общепринятой практикой является вычисления нескольких конверсий:
\begin{itemize}
\item $ClickRate = \frac{Clicks}{Shows}$ - доля показов приложения пользователю, которая привела к клику, переводящему на страницу установки.
\item $InstallRate = \frac{Installs}{Clicks}$ - доля переходов на старницу установки, завершившияся установкой приложения.
\end{itemize}
Так же часто вычисляют величину $InstallPerShow = ClickRate * InstallRate$ - доля показов, окончившихся установкой приложения.
Очевидно, что чем больше каждая из этих величин, тем лучше работает рекомендательная система, однако, встает вопрос о корректном сравнении нескольких различных рекомендательных систем.
Например, даны 2 рекомендательные системы $A$ и $A'$. Система $A$ проработала один день и 3 из 100 сформированных рекомендаций привели к установке. На следующих день работала $A'$, показав результаты в 4 установки на 100 показов. Не смотря на то, что $A'$ имеет большую конверсию, относительно $A$, однозначно утверждать о превосходстве $A'$ над $A$ пока нельзя, т.к. нет уверенности, что наблюдаемые изменения конверсии статистически значимы.

\subsection{A/B тестирование}
Пусть даны 2 рекомендательные системы $A$ и $A'$, а так же $\mathbb{U}$ - множество всех пользователей. Разобьем множество $\mathbb{U}$ случайным образом на 2 непересекающихся подмножества $\mathbb{A}$ и  $\mathbb{B}$ таким образом, что бы $\forall e \in U \textit{P}\{e \in \mathbb{A}\} = \textit{P}\{e \in \mathbb{B}\} = \frac{1}{2}$.
Для всех пользователей $a \in \mathbb{A}$ будем возвращать результаты, сформированные алгоритмом $A$, для остальных - сформированные алгоритмом $A'$.

Подсчитаем интересующую нас коверсию за каждый день исследования для обоих алгоритмов. В итоге получатся 2 выбоки: $a = (a_1, \dots, a_m)$ и $a' = (a'_1, \dots, a'_n)$ из распределений $F(x)$ и $G(x)$ соответственно.

Проверим нулевую гипотезу $\mathcal{H}_0: \textit{P}\{a < a'\} = 1/2$. Это можно сделать с помощью критерия Уилкоксона-Манна-Уитни.

Перед использованием данного критерия необходимо принять следующие гипотезы:
\begin{itemize}
\item выбоки $a$ и $a'$ - простые, объединенная выборка независима.
\item распределения $F(x)$ и $G(x)$ непрерывны.
\end{itemize}
Других ограничений на распределения (например, симметричность) не накладывается.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{исходные выборки $a$ и $a'$, $\alpha$ - уровень значимости}
\KwResult{Принимается ли гипотеза $\mathcal{H}_0$ при уровне значимости $\alpha$}

Построить вариационный ряд для объединенной выбоки, состоящий из элементов выборки  $a$ и $a'$. $y^{(1)} \le y^{(2)} \le \dots \le y^{(m + n)}$.

Найти ранги $r(a_i), r(a'_i)$ всех элементов из обеих выборок в общем вариационном ряду.

$R_{a} = \sum_{i=1}^{m}r(a_i)$ - суммарный ранг для выборки $a$

$R_{a'} = \sum_{i=1}^{n}r(a'_i)$ - суммарный ранг для выборки $a'$

$U_{a} = mn + \frac{1}{2}m(m+1) - R_{a}$ - статистика Манна-Уитни

$U_{a'} = mn + \frac{1}{2}n(n+1) - R_{a'}$ - статистика Манна-Уитни

$U = \min\{U_{a}, U_{a'}\}$

\% проверим гипотезу против альтернативы $\mathcal{H}_1 = \textit{P}\{a < a'\} \neq 1/2$

\If{$U \notin [U_{\frac{\alpha}{2}}, U_{1 - \frac{\alpha}{2}}]$}{
	\textbf{return} False
}

\% проверим гипотезу против альтернативы $\mathcal{H}'_1 = \textit{P}\{a < a'\} > 1/2$

\If{$U_a > U_{1 - \alpha}$}{
	\textbf{return} False
}

\% проверим гипотезу против альтернативы $\mathcal{H}'_1 = \textit{P}\{a < a'\} < 1/2$

\If{$U_{a'} > U_{1 - \alpha}$}{
	\textbf{return} False
}
\textbf{return} True
\caption{Использование критерия Уилкоксона-Манна-Уитни.}
\label{alg:MWW}
\end{algorithm}

Если гипотеза $\mathcal{H}_0$ принимается, то выводов о том, какой из алгоримов лучше сделать нельзя, в противном случае, можно одназначно указать, какой из представленных алгоритмов лучше с точки зрения выбранной конверсии.

\subsection{Смешивание результатов}
Не всегда есть возможность равномерно разбить пользователей на 2 множества и возвращать им разные результаты. Альтернативным способом сравнения двух алгоритмоя является использование в качестве рекомендаций, показываемых пользователю, смеси результатов работы обоих алгоритмов.

Пусть даны результаты работы двух различных алгоритмов формирования рекомендаций $A = (a_1, a_2, \dots)$ и $B = (b_1, b_2, \dots)$. Напомню, что $A$ и $B$ упорядочены по релевантности (или вероятности установки). Очевидный подход к смешиванию результатов - сбалансированное смешивание.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{$A, B$ - результаты работы различных алгоритмов формирования рекомендаций}
\KwResult{$I$ - результат после смешивания}
$I = \emptyset, k_a = 1, k_b = 1$

$AFirst = RandBit()$ -- определяем случайным образом приоритет у каждого результата

\While{$(k_a < |A|) \& (k_b < |B|)$}{
	\eIf{$(k_a < k_b) | ((k_a = k_b) \& (AFirst = 1))$}{
		\If{$A[k_a] \notin I$} {
			$I = I + A[k_a]$
		}
		$k_a = k_a + 1$
	}{
		\If{$B[k_b] \notin I$} {
			$I = I + B[k_b]$
		}
		$k_b = k_b + 1$
	}
}
\textbf{return} $I$
\caption{Сбалансированное смешивание (balanced Interleaving).}
\label{alg:BI}
\end{algorithm}

Такой подход к смешиванию результатов гарантирует, что среди $k$-лучших рекомендаций в $I$ всегда содержатся $k_a$-лучших рекомендаций из $A$ и $k_b$-лучших рекомендаций из $B$, при этом, $|k_a-k_b| \le 1$.

Предположим, что пользователь просматривает рекомендации снизу вверх [T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Transactions on nformation], а число рекомендованных приложений, которое увидит пользователь $l$ - зафиксированно и заранее известно. Т.е. у пользователя есть $l$ различных возможностей для клика, а число приложений, сформированных с помощью алгоритма $A$ и $B$ одинаково. Если пользователь будет случайно кликать по всем представленным рекомендациям, то он имеет равные шансы кликнуть как на приложение, ассоциированное с алгоритмом $A$, так и на приложение, ассоциированное с $B$. Т.е. если мы будем наблюдать большее абсолютное число кликов по приложениям, подобранным алгоритмом $A$, чем по приложениям, подобранным алгоритмом $B$, то можно делать выводы об относительном качестве двух алгоритмов.

Оформим процедуру принятия решения в виде алгоритма:


\begin{algorithm}[H]
\SetAlgoLined
\KwData{$A, B$ - результаты работы различных алгоритмов формирования рекомендаций, $I$ - результат BalanceInterleaving($A, B$), 
$(c_1,c_2,\dots)$ - номера приложений в выдаче $I$, по которым произошел клик}
\KwResult{$C$ - метка предпочтительного класса}
$l \approx c_{max} = \max\{c_1,c_2,\dots\}$ [ T. Joachims. Evaluating retrieval performance using clickthrough data. In J. Franke, G. Nakhaeizadeh, and I. Renz, editors, Text Mining. Physica Verlag, 2003]

$k = \min\{j: (i_{c_{max}} = a_j) | (i_{c_{max}} = b_j)\}$

$h_a = |{c_j: i_{c_j} \in (a_1, \dots, a_k)}|$

$h_b = |{c_j: i_{c_j} \in (b_1, \dots, b_k)}|$

\If{$h_a > h_b$} {
\textbf{return} 'A'
}
\If{$h_a < h_b$} {
\textbf{return} 'B'
}

\textbf{return} NULL

\caption{Алгоритм выбора предпочтительного алгоритма формирования рекомендаций для конкретной выдачи.}
\label{alg:CBI}
\end{algorithm}

Приведенный выше алгоритм позволяет апостериори понять, какой из использованных алгоритмов формирования рекомендаций.
Применим данную процедуру ко всему множеству известных данных, в результате получится выборка, содержащая метки алгоритмов формирования рекомендаций. Далее с помощью, анпример, биномиального теста можно определить, существуют ли различия между частотами появления меток различных алгоритмов. Если различия значимы, то алгоритм, чья метка имеет большую частоту признается лучшим.

Заметим, что апроксимация числа приложений, которое увидит пользователь, в виде $l \approx c_{max} = \max\{c_1,c_2,\dots\}$, может потенциально приводить к смещенным результатам, в случае использования сбалансированного смешивания. Например, в случаях, когда алгоритмы возвращают очень похожие списки рекоммендаций. Предположим, что $A = (a, b, cб в)$, а $B = (b, c, d, a)$, в случае сбалансированного смешивания, равновероятны два результата: $I=(a, b, c, d)$ или $I=(b, a, c, d)$. Заметим, что в обоих случаях, пользователь, равномерно кликающий на все приложения, создаст преимщество для алгоритма $B$.

Рассмоотрим альтернативных алгоритм смешивания, не страдающий от указанной проблемы.


\begin{algorithm}[H]
\SetAlgoLined
\KwData{$A, B$ - результаты работы различных алгоритмов формирования рекомендаций}
\KwResult{$I$ - результат после смешивания}
$I = \emptyset, TeamA = \emptyset, TeamB = \emptyset$

\While{($\exists i : A[i] \notin I) \& (\exists j: B[j] \notin I)$} {
	\eIf{$(|TeamA| < |TeamB|) | ((|TeamA| = |TeamB|) \& (RandBit() = 1))$} {
		$k=\min_i\{i: A{i}\notin I\}$
		
		$I = I + A[k]$
		
		$TeamA = TeamA \cup \{A[k]\}$
	} {
		$k=\min_i\{i: B{i}\notin I\}$
				
		$I = I + B[k]$
		
		$TeamA = TeamA \cup \{B[k]\}$
	}
}
\textbf{return} $I, TeamA, TeamB$
\caption{Алгоритм двух капитанов (team-draft interleaving).}
\label{alg:BI}
\end{algorithm}

Основным отличием от сбалансированного смешивания является то, что очерезность выбора элементов может случайным образом меняться, когда из $A$ и $B$ изъято по одинаковому кол-ву приложений.

Заметим, что теперь справедливы следующие равенства:
\begin{equation*}
h_a = |\{c_j: i_{c_j} \in TeamA\}|
\end{equation*}
\begin{equation*}
h_b = |\{c_j: i_{c_j} \in TeamB\}|
\end{equation*}

\chapter{Задача объяснения рекомендаций}
\section{Формальная постановка задачи}
Пусть дан некоторый алгоритм построения рекомендаций $A$, множество текстовых меток $\mathbb{L}$, множество пользователей $\mathbb{U} = \{x_0, \dots, x_{UserCount}\}$, представленных признаковым описанием и множество приложений  $\mathbb{I} = \{y_0, \dots, y_{AppCount}\}$, так же представленных признаковыми описниями.

Задача: найти отображение $D: \mathbb{A} \times \mathbb{I} \to \mathbb{S}$, сопоставляющее паре пользователь-приложение при фиксированном алгоритме $A$ текстовую метку, такое, что  $ClickRate \to \max$.

Т.е. необходимо для каждой конкретной рекомендации найти человеко-интерпретируемое объяснение. Как описывалось ранее, принимается гипотеза, что правильное объяснение рекомендации способно увеличить конверсию.
\section{Гипотеза о роли признака}
Примем следующую гипотезу: признак, вносящий наибольший вклад в предсказанную релевантность рекомендации, сильно коррелирует с причиной рекомендации.
Т.е. если дано отображение $M': \mathbb{F} \to \mathbb{S}$, где $\mathbb{F}$ - ножество признаков, то задача объяснения рекомендаций сводится к выявлению признака, вносящего наибольший вклад в предсказанную релевантность рекомендации.

В общем случае, можно рассмотреть отображение $M: \mathbb{F} \times \dots \mathbb{F} \to \mathbb{S}$. Т.е. можно выявлять не один признак, а группу признаков.

На практике количество призаков может быть очень велико (в рассматриваемой задаче около 500 признаков), поэтому будем рассматривать отображение $M$ на некотором подмножестве $\mathbb{F} \times \mathbb{F}$, т.е. часть признаков оставим без интерпретации.

Приведем пример отображения $M$. Пусть среди признаков, описывающих пару пользователь-приложение есть следующие признаки:
\begin{itemize}
\item $Friends: \mathbb{U} \times \mathbb{I} \to [0, 1]$ - доля друзей пользователя, у которых установлено данное приложение.
\item $Rating: \mathbb{U} \times \mathbb{I} \to [0, 5]$ - рейтинг приложения в магазине.
\end{itemize}
$M(Friends) = \textit{'Ваши друзья пользуются этим приложением'}$. Если приложение является каким-либо чатом или социальной сетью, то это может помочь убедить пользователя поставить прилоежние.
$M(Rating) = \textit{'У приложения отличный рейтинг'}$.
Для остальных $e \in \mathbb{F} \times \mathbb{F}$ отображение не определено.

Очевидно, что формулировка текста рекомендации может влиять на конверсию. Изучение влияния текста рекомендации на результат остается за рамками данной работы, поэтому, в рамках работы, отображение $M$ будет одинаковым для всех алгоритмов интерпретации.
\section{Алгоритмы формирования объяснений}
\subsection{Наивный алгоритм для Factorization Machines}

\begin{algorithm}[H]
\SetAlgoLined
\KwData{$\mathbb{F} = \{f_0, \dots f_{FactorsCount}\}$ - мн-во признаков, $u \in \mathbb{U}$ - пользователь, $a \in \mathbb{I}$ - приложение, $A$ - обученная модель Factorization Machines}
\KwResult{ i - номер признака, вносящего наибольший положительный вклад}

$maxContribution = 0$;

$idx = +\infty$

\For{$i = 0, \dots, FactorsCount$} {

contirbution = $A(Concat(f_0, \dots, f_{FactorsCount})) - A(Concat(f_0, \dots, f_{i-1}, 0, f_{i+i}, \dots, f_{FactorsCount}))$

\If{$contirbution > maxContribution$}{

$idx = i$

$maxContribution = contribution$

}
}

\textbf{return} $idx$
\caption{Псевдокод наивного определения вклада признака.}
\label{alg:naiveAI}
\end{algorithm}

Предложенный пособ обладает очевидным недостатком, он полагается на абсолютные значения изменения предсказанной вероятности. Но некоторые признаки всегда будут вносить больший абсолютный вклад, чем другие. Например, это может быть вызвано разной нормировкой различных признаков или тем, что одни признаки априори содержат больше информации, чем другие. Это может приводить к слишком однообразным объяснениям рекомендаций, если среди признаков есть несколько, содержащих гораздо больше информации, чем остальные.
\subsection{Оптимизация наивного алгоритма}
Введем гипотезу, что значение признака в данной рекомендации зависит не от абсолютного значения вклада данного признака в вероятность (как в базовом подходе), а от того, насколько больше этот вклад в конкретном случае, чем в среднем. Т.е. предлагается сравнивать не абсолютные значения прироста предсказанной вероятности, а относительные приросты для каждого признака. Такой подход позволяет нивилировать различные нормировки признаков, а так же их априорную информативность.

Введем обозначение: 
\begin{equation*}
c_{f_i}(x) = A(Concat(f_0, \dots, f_n)) - A(Concat(f_0, \dots, f_{i-1}, 0, \dots, 0, f_{i+i}, \dots, f_{FactorsCount})).
\end{equation*}
В качестве первого приближения, предположим, что значения $c_{f_i}$ распределены нормально $c_{f_i}\sim\mathcal{N}(\mu_{f_i}, \sigma_{f_i}^2)$.
Для оценки параметров $\mu_{f_i}, \sigma_{f_i}^2$ воспользуемся методом максимального правдоподобия, согласно которому:
\begin{equation*}
\begin{cases}
\tilde{\mu}_{f_i} = \overline{X} &\text{--- выборочное среднее} 
\\ \tilde{\sigma}_{f_i}^2 = \overline{S^2} &\text{ --- выборочная дисперсия}
\end{cases}
\end{equation*}
Выбрку для оценки парметров можно получить с помощью простого вероятностного сэмплирования мн-ва $\mathbb{U} \times \mathbb{I}$.


\begin{algorithm}[H]
\SetAlgoLined
\KwData{$\mathbb{F} = \{f_0, \dots f_{FactorsCount}\}$ - мн-во признаков, $u \in \mathbb{U}$ - пользователь, $a \in \mathbb{I}$ - приложение, $A$ - обученная модель Factorization Machines, $\mathbb{X} \subset \mathbb{U} \times \mathbb{I}$ - репрезентативная выборка пар пользователь-приложение}
\KwResult{ i - номер признака, вносящего наибольший положительный вклад}

$maxContribution = 0$;

$idx = +\infty$

\For{$i = 1, \dots, FactorsCount$} {

// оцениваем параметры распределения прироста вероятности от $i$-го признака

$\tilde{\mu}_{f_i}, \tilde{\sigma}_{f_i}^2 = MaxLikelihood(\mathbb{X}, i)$

contirbution = $\frac{c_{f_i}(x) - \tilde{\mu}_{f_i}}{\tilde{\sigma}_{f_i}}$

\If{$contirbution > maxContribution$}{

$idx = i$

$maxContribution = contribution$

}
}

\textbf{return} $idx$
\caption{Псевдокод определения признака с наибольшим относительным вкладом.}
\label{alg:normalAI}
\end{algorithm}

\subsection{Алгоритм объяснения для деревьев решения}

Заметим, что предложенные алгоримты для выявлении силы признака можно адаптировать и для ансамблей деревьев решений, в случае, если будет предложен способ игнорировать какой-либо признак при вычислении вероятности.

Очевидно, что подход с занулением какого-либо признака, как в случае, с Factoriazation Machines не сработает. Попробуем удалить все узлы, условия в которых используют указанный признак. Напомню, что в данной работе рассматриваются исключительно забывчивые деревья, однако, данные рассуждения можно модифицировать для произвольных деревьев решений. Т.к. деревья забывчивые, то на каждом уровне используется одинаковое условие, что означает, что необходимо удалять целые уровни.

Напомню, что забывчивые деревья можно представлять в виде пары \\
$(\{Condition\}_{i=1}^{depth}, \{Score, Points\}_{i=0}^{2^{depth - 1}})$. После удаления уровня должно получиться так же забывчивое дерево, опирающиеся на теже условия, кроме тех, что используют указанный признак.
Предположим, что условие $Condition_i$ использует признак $f \in \mathbb{F}$. Удалим условие  $Condition_i$ из оригинального списка условий, в результате получится список условий, соответствующий описанным требованиям. Теперь каждой возможной комбинации условий соответствует ровно 2 листа оригинального дерева, в качестве нового листа будем брать объединение старых листов (данная опреция описана в алгоритме \ref{alg:MLFS}).

Оформим данные рассуждения в виде готового алгоритма:

\begin{algorithm}[H]
\SetAlgoLined
\KwData{$OriginTree = (Conditions, Leaves)$ - оригинальное дерево, представленное парой условия-решения, $f_i \in \mathbb{F}$ - признак, который надо исключить из рассмотрения}
\KwResult{ $TruncatedTree$ - новое дерево, построенное на основе $OriginTree$, но игнорирующее признак $f_i$}
$depth = len(Conditions)$ -- сохраняем оригинальную глубину дерева

$idx = findFeature(Conditions, f_i)$ -- ищем индекс условия, использующего признак $f_i$, если условий несколько, то возвращаем индекс первого

\eIf{$idx \ge 0$} {
	$newConditions = remove(Conditions, idx)$ -- удалим найденное условие из списка используемых в новом дереве
	
	\For{$j \in [0, 2^{depth-1} - 1]$} { -- заполним массив newScores
	
		$idxLeft = insertBit(j, idx, 0)$ -- операция $insertBit(n, i, b)$ вставляет бит $b$ в число $n$ на позицию $i$, при этом длина числа в битовом представлении увеличивается на 1
		
		$idxRigth = insertBit(j, idx, 1)$
		
		$newLeaves[j] = merge(Leaves(idxLeft), Leaves(idxRigth))$
		
		\textbf{return} $RemoveFeature((newConditions, newLeaves), f_i)$
	}
} {
	\textbf{return} ((Conditions, Leaves)
}
\caption{Процедура удаления условия из забывчивого дерева принятия решений.}
\label{alg:ROTN}
\end{algorithm}

\begin{algorithm}[H]
\SetAlgoLined
\KwData{$L1, L2$ - два листа дерева}
\KwResult{$L$ - объединенный лист}
$PointsInL1 = getPoints(L1)$

$PointsInL2 = getPoints(L1)$

$TotalPoints = PointsInL1 + PointsInL2$

$TotalScore = \frac{PointsInL1}{TotalPoints}getScore(L1) + \frac{PointsInL2}{TotalPoints}getScore(L2)$

\textbf{return} $(TotalScore, TotalPoints)$
\caption{Процедура merge - объединения двух листов.}
\label{alg:MLFS}
\end{algorithm}

Теперь можно изменить способ вычисления $c_{f_i}(x)$ - абсолютного вклада признака $f_i$ следующим образом:

\begin{algorithm}[H]
\SetAlgoLined
\KwData{$Ensemble = \{(Tree_i, c_i)\}$- ансамбль забывчивых деревьев, $x$ - признаковое описание объекта, $f_i$ - удаляемый признак}
\KwResult{$c_{f_i}(x)$}

$Ensemble_i = \emptyset$

\For{$(Tree, c) \in Ensemble$} {

	$Ensemble_i \leftarrow (removeFeature(Tree, f_i), c)$ -- составляем новый ансамбль из деревьев с удаленным свойством $f_i$
	
}
 
$x' = remove(x, i)$ -- удалим признак $f_i$ из признакового описания

\textbf{return} $Ensemble(x) - Ensemble_i(x')$
\caption{Процедура вычисления величины $c_{f_i}(x)$ для ансамбля забывчивых деревьев}
\label{alg:CFIOT}
\end{algorithm}

\subsection{Отказ от гипотезы о нормальности распределения}
На рисунке $\ref{ris:contriball}$ представлены гистограммы вкладов различных признаков в итогувую веротяность. Выборка, исопльзованная при построении данных гистограмм была получена с помощью вероятностного семплирования пространства $\mathbb{U} \times \mathbb{I}$. Размер выборки составляет порядка $10^5$ элементов.

Из рисунка $\ref{ris:contriball}$ видно, что нельзя считать, что  $c_{f_i} \sim \mathcal{N}(\mu_{f_i}, \sigma_{f_i}^2)$.

\begin{figure}[pH]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.8\linewidth]{182_distr} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.9\linewidth]{76_distr} \\ б)}
\end{minipage}
\begin{minipage}[h]{1.\linewidth}
\center{\includegraphics[width=0.9\linewidth]{16_distr} \\ в)}
\end{minipage}
\caption{Гистограммы вкладов различных признаков}
\label{ris:contriball}
\end{figure}

Постараемся вообще отказаться от каких-либо предположений о форме распределения значений $c_{f_i}$.

Заменим обозначение $c_{f_i}$ на $x$.
Предположим, что $x \sim \mathcal{F}_{X}$, где $\mathcal{F}_{X}$ - эмпирическая функция распределения с плотностью распределения $f$. Можно вычислить значение величины 
\begin{equation*}
p_{c_{f_i}} = \textit{P}\{X \le x\} \equiv \mathcal{F}_X(x) \equiv \int_{-\infty}^{x}f(x)dx \equiv 1 - \int_{x}^{+\infty}f(x)dx
\end{equation*}
и использовать ее для сравнения вкладов, вносимых различными признаками.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{$\mathbb{F} = \{f_0, \dots f_{FactorsCount}\}$ - мн-во признаков, $u \in \mathbb{U}$ - пользователь, $a \in \mathbb{I}$ - приложение, $A$ - алгоритм классификации, $f$ - плотность распределения $\mathcal{F}_{X}$}
\KwResult{ i - номер признака, вносящего наибольший положительный вклад}

$maxProb = 0$;

$idx = +\infty$

\For{$i = 1, \dots, FactorsCount$} {

$c_{f_i} = getContribution(A, f_i, u, a)$ -- получение вклада признака $f_i$

$curProb = 1 - \int_{x}^{+\infty}f(c_{f_i})dx$

\If{$curProb > maxProb$}{

$idx = i$

$maxProb = curProb$

}
}

\textbf{return} $idx$
\caption{Определения признака с наибольшим относительным вкладом в случае произвольного распределения.}
\label{alg:AFAI}
\end{algorithm}

Заметим, что в том случае, когда $\mathcal{F} \sim \mathcal{N}(\mu, \sigma^2)$ алгоритмы \ref{alg:normalAI} и \ref{alg:AFAI} дают одинаковые результаты.

\subsubsection{Восстановление плотности распределения}
[ПРО СТОХАСТИЧЕСКИЙ ЕМ ЗДЕСЬ: Gilles Celeux, Didier Chauveau, Jean Diebolt. On Stochastic Versions of the EM Algorithm. [Research Report] RR-2514, 1995]

Попробуем приблизить искомое распределение с помощью смеси распределений:

\begin{equation*}
p(x) = \sum_{j=1}^{k}\omega_jp_j(x), \sum_{j=1}^{k}\omega_j = 1, \omega_j \ge 0, 
\end{equation*}
где $p_k(x, \theta_k)$ - функция правдоподобия $j$-й компоненты смеси, $\omega_j$ - ее априорная вероятность. Функции правдоподобия принадлежат параметрическому семейству распределений $\phi(x; \theta)$ и отличаются только значением параметра $p_j(x) = \phi(x; \theta_j)$.
Задача: зная число компонент смеси $k$, имея выборку $X^m$ случайных и независимых событий из смеси $p(x)$ и зная функцию $\phi$ оценить параметры $\Theta = (\omega_1, \dots, \omega_k, \theta_1, \dots, \theta_k)$.

Классическим подходом к решению данной задачи, является использование EM-алгоритма.

\textbf{E-шаг (expctation).}
Обозначим через $p(x, \theta_j)$ плотность веротяности того, что объект $x$ получен и $j$-й компоненты смеси. По формуле условной веротяности:
\begin{equation*}
p(x, \theta_j) = p(x)P(\theta_j|x) = \omega_jp_j(x).
\end{equation*} 

Введем обозначение $g_{ij} \equiv P(\theta_j|x_i)$ - апостериорная вероятность того, что объект $x_i$ получен из $j$-й компоненты смеси. Обозначим $G=(g_{ij})_{m \times k}$. Т.к. каждый объект принадлежит какой-либо компоненте:
\begin{equation*}
\forall i \in (1, \dots, m) \sum_{j=1}^{k}g_{ij} = 1.
\end{equation*}

Зная параметры помпонент $\omega_j, \theta_j$, можно вычислить $g_{ij}$ по формуле Байеса:
\begin{equation*}
\forall i,j g_{ij} = \frac{\omega_jp_j(x_i)}{\sum_{s=1}^{k}\omega_sp_s(x_i)}.
\end{equation*}

\textbf{M-шаг (maximization).}
Постараемся максимизировать логарифм правдоподобия:
\begin{equation*}
Q(\Theta) = \ln \prod_{i=1}^{m}p(x_i) = \sum_{i=1}^{m}\ln \sum_{j=1}^{k} \omega_jp_j(x_i) \to \max_{\Theta}
\end{equation*}
при ограничении $\sum_{j=1}^{m} = 1$. Запишем лагранжиан данной оптимизационной задачи:
\begin{equation*}
L(\Theta, X^m) = \sum_{i=1}^{m}\ln (\sum_{j=1}^{k}\omega_jp_j(x_i)) - \lambda(\sum_{j=1}^{k}\omega_j - 1).
\end{equation*}
Приравняем производную лагранжиана по $\omega_j$ к нулю:
\begin{equation*}
\frac{\partial L}{\partial \omega_j} = \sum_{i=1}^{m} \frac{p_j(x_i)}{\sum_{s=1}^{k}\omega_sp_s(x_i)} -\lambda = 0, \forall j \in (1, \dots, k).
\end{equation*}
Умножим обе части на $\omega_j$, просуммируем все $k$ равенств и изменим порядок суммирования:
\begin{equation*}
\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{\omega_jp_j(x_i)}{\sum_{s=1}^{k}\omega_sp_s(x_i)} = \lambda \sum_{j=1}^{k}\omega_j,
\end{equation*}
откуда следует, что $\lambda = m$. Учитывая этот факт:
\begin{equation*}
\omega_j = \frac{1}{m} \sum_{i=1}^{m}\frac{\omega_jp_j(x_i)}{\sum_{s=1}^{k}\omega_sp_s(x_i)]} = \frac{1}{m}\sum_{j=1}^m g_{ij}, \forall j \in (1, \dots, k).
\end{equation*}
Приравняем производную лагранжиана по $\theta_j$ к 0, понятно, что $p_j(x) \equiv \phi(x' \theta_j)$:
\begin{eqnarray}
\frac{\partial L}{\partial \theta_j} = \sum_{i=1}^{m} \frac{\omega_j}{\sum_{s=1}^{k}\omega_sp_s(x_i)} \frac{\partial }{\partial \theta_j} p_j(x_i) = 
\sum_{i=1}^{m} \frac{\omega_j p_j(x_i)}{\sum_{s=1}^{k}\omega_sp_s(x_i)} \frac{\partial }{\partial \theta_j} \ln p_j(x_i) = \\
\sum_{i=1}^{m}g_{ij} \frac{\partial }{\partial \theta_j} \ln p_j(x_i) = \frac{\partial }{\partial \theta_j} \sum_{i=1}^{m}g_{ij}  \ln p_j(x_i) = 0, \forall j \in (1, \dots, k).
\end{eqnarray}

Полученное условие совпадает с необходимым условием максисмума в задаче максимизации взвешенного правдоподобия
\begin{equation*}
\theta_j = \arg \max_{\theta} \sum_{i=1}^{m}g_{ij}  \ln\phi(x_i; \theta).
\end{equation*}


\begin{algorithm}[H]
\SetAlgoLined
\SetKwRepeat{Do}{do}{while}
\KwData{$X^m = (x_1, \dots, x_m)$ - выборка, $k$ - число компонент в смеси, $\Theta=(\omega_j,\theta_j)_{j=1}^k$ - начальное приближение параметров смеси, $\delta$ - параметр критерия остановки}
\KwResult{$\Theta$ - оптимальный вектор параметров смеси}
\Do{$\max_{i,j}|g_{ij} - g_{ij}^0| > \delta$}{
\textbf{E-шаг:}

	\For{$i \in  (1, \dots, m), j \in  (1, \dots, k)$} {
		$g_{ij}^0 = g_{ij}$
		
		$g_{ij} = \frac{\omega_j \phi(x_i; \theta_j)}{\sum_{s=1}^{k} \omega_s \phi(x_i; \theta_s)}$
	}

	\textbf{M-шаг:}
	
	\For{$j \in  (1, \dots, k)$} {
		$\theta_j = \arg \max_{\theta} \sum_{i=1}^{m}g_{ij}  \ln\phi(x_i; \theta)$
	}
}

\textbf{return} $(\omega_j, \theta_j)_{j=1}^k$
\caption{Классический EM-алгоитм.}
\label{alg:EM}
\end{algorithm}

Приведенный алгоритм обладает рядом недостатков, одним из которых является знание числа компонент в смеси  $k$. Рассмотрим алгоритм, автоматически определяющий необходимое число компонент:

\begin{algorithm}[H]
\SetAlgoLined
\SetKwRepeat{Do}{do}{while}
\KwData{$X^m = (x_1, \dots, x_m)$ - выборка, $\delta$ - параметр критерия остановкиб $R$ -  максимальный допустимый разброс правдоподобия, $m_0$ - минимальная длина выборки по которой можно восстановить плотность}
\KwResult{$k$ - число компонент, $\Theta$ - оптимальный вектор параметров смеси}

$\theta_1 =  \arg \max_{\theta} \sum_{i=1}^{m} \ln\phi(x_i; \theta)$ -- начальное приближение

$\omega_1 = 1, k = 1$

\For{$k \in (2, 3, \dots)$} {
	$\mathbb{U} = \{x_i \in X^m: p(x_i) < \max_j p(x_j) / R\}$ -- выбираем объекты с низким правдоподобием
	
	\If{$|\mathbb{U}| < m_0$} {
		\textbf{exit}
	}
	
	$\theta_k =  \arg \max_{\theta} \sum_{x_i \in \mathbb{U}} \ln\phi(x_i; \theta)$ -- начальное приближение для $k$-й компоненты
	
	$\omega_k = \frac{|\mathbb{U}|}{m}$
	
	$\omega_j = \omega_j(1-\omega_k), \forall j \in (1, \dots, k-1)$
	
	$EM(X^m, k, \Theta, \delta)$
}
\caption{EM-алгоитм с последовательным добавление компонент.}
\label{alg:KEM}
\end{algorithm}

Заметим, что минимизируемый функционал $Q(\Theta)$ может быть не выпуклым и иметь большое количество локальных экстремумов, что делает классический алгоритм сильно зависимым от качества начального приближения. Оптимизация, позволяющая значительно уменьшить роль начального приближения называется SEM (стохастический EM).

Основным отличием SEM от классического EM является то, что вместо решения задачи максимизации взвешенного правдоподобия решается задача максимизацции обычного, не взвешенного правдоподобия:
\begin{equation*}
\theta_j = \arg \max_{\theta} \sum_{x_i \in X^{(g_j)}} \ln \phi(x_i; \theta),
\end{equation*}
где подвыборка $X^{(g_j)}$ генерируется из $X^m$ с помощью стохастического моделирования: каждый объект $x_i \in X^m$ попадает в $X^{(g_j)}$ с вероятностью $g_{ij}$.

Еще одним важным отличием является способ подбора числа $k$. Данный алгоритм начинает работу с заведомо большего числа компонент, чем может оказаться в смеси. Далее после очередного шага все компоненты, для которых справедливо $|X^{(g_j)}| \le m_0$ удаляются и число $k$ соответственно уменьшается.

К осоновным преимуществам SEM относят:
\begin{itemize}
\item малое число итераций для сходимости;
\item результаты практически не зависят от начального приближения
\item как правило, находится экстремум, близкий к глобальному.
\end{itemize}

Т.к. в общем случае никаких гипотез о форме компонент смеси составить нельзя, будем изкать результирующую плотность, как смесь нормальных плотностей. Результаты применения различных способов оценки плотности представлены на Рис.  $\ref{ris:cmp}$.

\begin{figure}[H]
\center{\includegraphics[width=0.8\linewidth]{cmp})}
\caption{Сравнение различных апроксимаций плотности}
\label{ris:cmp}
\end{figure}

\subsection{Усеченные распределения}
WARNING! ДАЛЕЕ ИДЕТ ПОТОК СОЗНАНИЯ, КОТОРЫЙ НАДО ХОРОШЕНЬКО ОБДУМАТЬ
ВЕРДИКТ ИК: ТРЕШ, УГАР, ВЫПИЛИВАЕМ

При анализе полученных гистограмм, можно заметить, что влияние большинства признаков зачастую ограничивается и сверху и снизу некоторыми константами, например, на Рис. $\ref{ris:somecontrib}$ видно, что вклад данного признака положительный для всех примеров, попавших в выборку.

\begin{figure}[H]
\center{\includegraphics[width=0.8\linewidth]{feature_distribution}}
\caption{Гистограмма вклада некоторого признака}
\label{ris:somecontrib}
\end{figure}

Данному наблюдению легко найти обоснование, если обратиться к структуре решающей функции, лежащей в основе конкретного классификатора.
Например, пусть дано забывчивое дерево $T = (\{f_i > \lambda_i\}_{i=1}^6, \{(\alpha_j, points_j)\}_{j=0}^{31})$. Пусть $T' = remove(T, 5) =  (\{f_i > \lambda_i\}_{i=1, i \ne 5}^6, \{(\alpha'_j, points'_j)\}_{j=0}^{15})$, где $remove$ - операция удаления признака, описанная ранее. Очевидно, что величина вклада, вносимого 5-м признаком может быть оценена сверху, как $\max_{i = 1, \dots, 31, j = 1, \dots, 15}(\alpha_i - \alpha'_j)$. Т.к. вклад от одного дерева ограничен, то и вклад от конечной суммы деревьев так же является ограниченным.

Рассмотрим Рис. $\ref{ris:fakecontrib}$, из графика плотности следует, что есть не нулевая вероятнось того, что вклад признака будет отрицательным, однако, данное утверждение не подтвердилось ни для одного элемента из выборки, по которой была построена приведеннная гисторамма.
\begin{figure}[H]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.8\linewidth]{feature_distribution}}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.8\linewidth]{feature_estimation}}
\end{minipage}
\caption{Гистограмма вклада некоторого признака и оценка для его плотности}
\label{ris:fakecontrib}
\end{figure}

Пусть дана выборка $X^m$, найдем числа $a = \min_{x \in X^m}x$ и $b = \max_{x \in X^m}x$. Описанной проблемы можно попытаться избежать, если в качестве компонент смеси брать распределения, не равные нулю исключительно на компакте $[a, b]$. 

Пусть $f$ - произвольная непрерывная функция плотности, заданная на $\mathbb{R}$, тогда 
\begin{equation*}
f'(x) = 
 \begin{cases}
   \frac{f(x)}{\int_{a}^{b}f(x)dx} &\text{if $x \in [a, b]$}\\
   0 &\text{if $x \notin [a, b]$}
 \end{cases}
\end{equation*}

 - так же является функцией плотности, заданной на $\mathbb{R}$, но все не нулевые значения которой расположены внутри компакта $[a, b]$. Функции плотностей, полученные описанным образом называются усеченными.
 
Попробуем искать функцию распределение вклада некоторого признака как смесь усеченных нормальных распределений с параметрами $a, b$.

Для этого необходимо научиться решать задачу максимизации взвешенного правдоподобия: 
\begin{equation*}
(\mu_j, \sigma_j) = \arg\max_{\mu, \sigma}g(\mu, \sigma) 
\end{equation*}
\begin{equation*}
g(\mu, \sigma) = \sum_{i=1}^{m}g_{ij}\ln\frac{\mathcal{N}(x_i, \mu, \sigma)}{\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})}
\end{equation*}

Запишем частные производные функции $g$ по $\mu$ и $\sigma$:
\begin{equation*}
\frac{\partial}{\partial \mu}g(\mu, \sigma) = \sum_{i=1}^{m}g_{ij} ( \frac{(x - \mu) \mu}{\sigma^2} - \frac{\frac{\partial}{\partial \mu} \Phi(\frac{b-\mu}{\sigma}) - \frac{\partial}{\partial \mu} \Phi(\frac{a-\mu}{\sigma})}{\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})} )
\end{equation*}
\begin{equation*}
\frac{\partial}{\partial \mu}g(\mu, \sigma) = \sum_{i=1}^{m}g_{ij} ( \frac{(x - \mu) \mu}{\sigma^2} - \frac{-\frac{2\mu}{\sqrt{\pi}\sigma}e^{-\frac{(b-\mu)^2}{\sigma^2}} + \frac{2\mu}{\sqrt{\pi}\sigma}e^{-\frac{(a-\mu)^2}{\sigma^2}}}{\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})} )
\end{equation*}
\begin{equation*}
\frac{\partial}{\partial \sigma}g(\mu, \sigma) = \sum_{i=1}^{m}g_{ij} ( -\frac{1}{\sigma} + \frac{(x-\mu)^2}{\sigma^3} -\frac{\frac{2(\mu - b)}{\sqrt{\pi}\sigma^2}e^{-\frac{(b-\mu)}{\sigma}} - \frac{2(\mu - a)}{\sqrt{\pi}\sigma^2}e^{-\frac{(a-\mu)}{\sigma}}}{\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})} )
\end{equation*}

Т.к. в случае смеси усеченных нормальных распределений функция взвешенная правдоподобия является линейной комбинацией выпуклых функций, то максимум можно найти с помощью градиентного метода, используя выражения для частных производных.

\chapter{Результаты испытаний}
\section{Предварительный анализ}
Прежде чем перейти непосредственно к результатам A/B  тестирования рассмотрим подробнее результаты работы предолженных алгоритмов.

При описании наивного алгоритма (Алг. $\ref{alg:naiveAI}"$), была выдвинута гипотеза, что результаты могут быть слишком однообразными как из-за различной нормировки признаков, так и из-за различной априорной информативности. Что бы подтвердить данную гипотезу построим гистограмму, показывающую сколько признаков в какой доле случаев выделяются наивным алгоритмом.

\begin{figure}[H]
\center{\includegraphics[width=0.8\linewidth]{dummycoverage})}
\caption{Доля случаев, покрываемых n-признаками в случае наивного алгоритма}
\label{ris:dummycoverage}
\end{figure}

На Рис. $\ref{ris:dummycoverage}$ видно, что в более чем 25\% случаев в качестве "сильнейшего" признака выбирается один и тот же. А что бы покрыть 90\% случаев необходимо всего лишь 20 признаков из 415 рассмотренных.

Очевидно, что такие однообразные объяснения рекомендаций могут быстро надоесть пользователю и начать его раздражать, что приведет к немедленному ухудшению общего качества рекомендательной системы.

Аналогичные гистограммы для Алг. $\ref{alg:normalAI}$ и SEM-модификации Алг. $\ref{alg:AFAI}$ приведены на Рис.$\ref{ris:allcoverage}$.

\begin{figure}[H]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.8\linewidth]{facepalm} \\ а) Нормальный алгоритм}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.8\linewidth]{facepalm} \\ б) Алгоритм для произвользого распределения}
\end{minipage}
\caption{Доля случаев, покрываемых n-признаками для различных алгоритмов}
\label{ris:allcoverage}
\end{figure}

 TODO:  разница между вкладами i-м и (i+1)-м. ДЛя обоснования выбора числа признаков, по которым строится отображение. Так же должно снять вопросы про точность численного интегрирования.

\chapter{Результаты}
\section{Улучшение качества рекомендаций}
\section{Оценка трудоемкости формрирования объяснений}
\section{Способы обобщения предолженных алгоритмов для других рекомендательных систем}

\chapter{Охрана труда}
\section{Введение}
Охрана труда представляет собой систему законодательных актов, социально-экономических, организационных, технических и лечебно-профилактических мероприятий и средств, обеспечивающих безопасность, сохранение здоровья и работоспособности человека в процессе труда [43]. Охрана труда выявляет и изучает возможные причины производственных несчастных случаев, профессиональных заболеваний, аварий, взрывов, пожаров и разрабатывает систему мероприятий и требований с целью устранения этих причин и создания, безопасных и благоприятных для человека условий труда. Полностью безопасных и безвредных производственных процессов не существует. Задача охраны труда – свести к  минимуму вероятность поражения или заболевания работающего с одновременным обеспечением комфорта при максимальной производительности труда.

Сложность стоящих перед охраной труда задач требует использования достижений и выводов многих научных дисциплин, прямо или косвенно связанных с задачами создания здоровых и безопасных условий труда. Так как главным объектом охраны труда является человек в процессе труда, то при разработке требований производственной санитарии используются результаты исследований ряда медицинских и биологических дисциплин. Особо тесная связь существует между охраной труда, научной организацией труда, инженерной психологией, технической эстетикой и эргономикой. Эргономика изучает трудовую деятельность в комплексе, в ней объединяются научные дисциплины, развивавшиеся прежде независимо друг от друга. 

Эргономика – научная дисциплина, изучающая трудовые процессы с целью создания оптимальных условий труда, что способствует увеличению его производительности, а также обеспечивает необходимые удобства и сохраняет силы, здоровье и работоспособность человека. В последние годы много новых идей возникло в связи с рассмотрением трудовой деятельности как процесса взаимодействия человека с машиной и более сложными системами управления. В связи с этим эргономику условно можно разделить на три подобласти: 
\begin{itemize}
\item Микро-эргономика – исследование и проектирование систем “человек – машина”. Сюда же включаются интерфейсы “человек-компьютер” (компьютер  рассматривается как часть машины) - как аппаратные интерфейсы, так и программные. Соответственно, “эргономика программного обеспечения” – это подраздел микро-эргономики. Сюда же относятся системы: “человек – компьютер – человек”, “человек – компьютер – процесс”, “человек – программа, ПО, ОС”.
\item Миди-эргономика – исследование и проектирование систем “человек – рабочая группа, коллектив, экипаж, организация”, “коллектив – машина”, “человек – сеть, сетевое сообщество”, “коллектив – организация”. Сюда входит и проектирование организаций, и планирование работ, и обитаемость рабочих помещений, и гигиена труда, и проектирование залов с дисплеями общего пользования, проектирование интерфейсов сетевых программных продуктов, и многое, многое другое. Исследуется взаимодействие на уровне рабочих мест и производственных задач.
\item Макро-эргономика – исследование и проектирование систем “человек – социум, общество, государство”, “организация - система организаций”.
\end{itemize}

Компьютер стал неотъемлемой частью жизни практически каждого человека. Сейчас сложно найти сферу деятельности, которая так или иначе не соприкасалась с вычислительными машинами. Комфортность труда и высокая производительность на рабочем месте оператора зависит от правильного выбора основного и вспомогательного оборудования, которое должно отвечать эргономическим требованиям. 

В создании благоприятных условий для повышения  производительности и уменьшения напряжения пользователя значительную роль играют факторы, характеризующие состояние окружающей  среды: размер и микроклимат помещения, уровень шума и вибрации в помещении.

\section{Психо-физиологические факторы}
В комплексе мероприятий по совершенствованию организации труда важная роль отводится внедрению научно-обоснованных режимов труда и отдыха, улучшению условий труда. Основная цель рационального труда и отдыха — поддержание работоспособности на оптимальном уровне. Необходимость чередования труда и отдыха обусловлена физио-логическими закономерностями и играет большую роль в поддержании трудового ритма. Работоспособность работника в течение рабочего дня не является величиной стабильной. Основные фазы работоспособности:
\begin{itemize}
\item вырабатывание и нарастающая работоспособность;
\item высокая, устойчивая работоспособность;
\item падение работоспособности в результате развивающегося утомления.
\end{itemize}

Оптимальный режим труда и отдыха должен включать паузы. При неблагоприятных условиях труда высокий уровень работоспособности составляет не менее 75\% рабочего времени. Период вырабатывания составляет не более 40 минут, а восстановительный период — не более 10-15 минут. Наибольшая работоспособность инженерно-технических работников наблюдается с 10 до 12 и с 16 до 18 часов. Рекомендуется делать перерывы по 8-10 минут каждые 2 часа в первой половине дня и 5-8 минут через каждый час во второй половине дня.

\section{Помещение}
Помещения с видеодисплейными терминалами (далее – ВДТ) и персональными электронно-вычислительными машинами (далее - ПЭВМ) должны иметь естественное и искусственное освещение [44]. Естественное освещение должно осуществляться через светопроемы, которые ориентированы преимущественно на север и северо-восток.

Расположение рабочих мест с ВДТ и ПЭВМ для взрослых пользователей в подвальных помещениях не допускается. Размещение рабочих мест с ВДТ и ПЭВМ во всех учебных заведениях и дошкольных учреждениях не допускается в цокольных и подвальных помещениях. В случаях производственной необходимости эксплуатация ВДТ и ПЭВМ в помещениях без естественного освещения может проводиться только по согласованию с органами и учреждениями Государственного санитарно – эпидемиологического надзора.

Площадь на одно рабочее место с ВДТ или ПЭВМ для взрослых пользователей должна составлять не менее 6,0 кв. м, а объем - не менее 20,0 куб. м. 

Производственные помещения, в которых для работы используются преимущественно ВДТ и ПЭВМ (диспетчерские, операторские, расчетные и др.), и учебные помещения (аудитории вычислительной техники, дисплейные классы, кабинеты и др.) не должны граничить с помещениями, в которых уровни шума и вибрации превышают нормируемые значения (механические цеха, мастерские, гимнастические залы и т.п.). Звукоизоляция ограждающих конструкций помещений с ВДТ и ПЭВМ должна отвечать гигиеническим требованиям и обеспечивать нормируемые параметры шума.

Помещения с ВДТ и ПЭВМ должны оборудоваться системами отопления, кондиционирования воздуха или эффективной приточно-вытяжной вентиляцией. Расчет воздухообмена следует проводить по избыткам тепла от машин, людей, солнечной радиации и искусственного освещения. Нормируемые параметры микроклимата, ионного состава воздуха, содержание вредных веществ в нем должны отвечать требованиям Санитарных правил. 

Для внутренней отделки интерьера помещений с ВДТ и ПЭВМ должны использоваться диффузно - отражающие материалы с коэффициентом отражения для потолка – 0,7 - 0,8; для стен – 0,5 - 0,6; для пола – 0,3 - 0,5. Полимерные материалы, используемые для внутренней отделки интерьера помещений с ВДТ и ПЭВМ, должны быть разрешены для применения органами и учреждениями государственного санитарно-эпидемиологического надзора. В дошкольных и всех учебных учреждениях, включая вузы, запрещается для отделки внутреннего интерьера помещений с ВДТ и ПЭВМ применять полимерные материалы (древесностружечные плиты, слоистый бумажный пластик, синтетические ковровые покрытия др.), выделяющие в воздух вредные химические вещества. 

Поверхность пола в помещениях эксплуатации ВДТ и ПЭВМ должна быть ровной, без выбоин, нескользкой, удобной для очистки и влажной уборки, обладать антистатическими свойствами.

\section{Микроклимат}
Микроклиматические условия устанавливаются ГОСТом 12.1.005-88 и СанПиНом 2.2.2.548-96. Оптимальные и допустимые параметры микроклимата приведены в Табл. $\ref{tabular:micro1}$ и Табл. $\ref{tabular:micro2}$ соответственно.

\begin{table} [H]
\label{tabular:micro1}
\begin{center}
\begin{tabular}{|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|}
\hline
Период года & Категория работ & Температура (град. C) & Относительная влажность (\%) & Скорость движения воздуха (м/с) \\
\hline
Холодный и переходный & Легкая 1а & 22 -- 24 & 40 -- 60 & 0,1 \\
\hline
Холодный и переходный & Легкая 1б & 21 -- 23 & 40 -- 60 & 0,1 \\
\hline
Теплый & Легкая 1а & 23 -- 25 & 40 -- 60 & 0,1 \\
\hline
Теплый & Легкая 1б & 22 -- 24 & 40 -- 60 & 0,2 \\
\hline
\end{tabular}
\end{center}
\caption{Оптимальные параметры микроклимата}
\end{table}

Примечания к Табл. $\ref{tabular:micro1}$:
\begin{itemize}
\item категории 1а относятся работы, производимые сидя и не требующие физического напряжения, при которых расход энергии составляет до 120 ккал/ч
\item к категории 1б относятся работы, производимые сидя, стоя или связанные с ходьбой и сопровождающиеся некоторым физическим напряжением, при которых расход энергии составляет от 120 до 150 ккал/ч
\end{itemize}

\begin{table} [H]
\label{tabular:micro2}
\begin{center}
\begin{tabular}{|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|p{0.15\linewidth}|}
\hline
Период года & Категория работ & Температура (град. C) & Относительная влажность (\%) & Скорость движения воздуха (м/с) \\
\hline
Холодный и переходный & Легкая & 19 -- 25 & $\le$ 75 & < 0,2 \\
\hline
Теплый & Легкая & Не более чем на $3^\circ$C выше средней температуры воздуха в 13 часов самого жаркого месяца, но не более $28^\circ$C & $24^\circ$C:<75
$25^\circ$C:<70
$26^\circ$C:<65
$27^\circ$C:<60
$28^\circ$C:<55
 & < 0,2 -- 0,5 \\
\hline
\end{tabular}
\end{center}
\caption{Допустимые параметры микроклимата}
\end{table}

\section{Уровень шума}
На рабочем месте пользователя источником шума является вычислительная машина, производящая постоянный шум. Шум предоставляет собой сочетание звуков, различных по интенсивности и частоте в частотном диапазоне 10-20 кГц, не несущих полезной информации. Шум вредно воздействует не только на органы слуха, но и на весь организм человека в целом через центральную нервную систему. Шум – причина преждевременного утомления, ослабления внимания, памяти. Во всех учебных и дошкольных помещениях с ВДТ и ПЭВМ уровень шума на рабочем месте не должен превышать 50 дБА. Шумящее оборудование (принтеры, сканеры, факсы и т.д.), уровни шума которого превышают нормированные, должно находиться вне помещения с ВДТ и ПЭВМ. 

Снизить уровень шума в помещениях с ВДТ и ПЭВМ можно использованием звукопоглощающих материалов с максимальными коэффициентами звукопоглощения в области частот 63 - 8000 Гц для отделки помещений (разрешенных органами и учреждениями Госсанэпиднадзора России). Дополнительным звукопоглощением служат однотонные занавеси из плотной ткани, гармонирующие с окраской стен и подвешенные в складку на расстоянии 15 - 20 см от ограждения. Ширина занавеси должна быть в 2 раза больше ширины окна.

\section{Рабочее место}
Под рабочим местом пользователя понимается не только стол, а пространство, где находится и работает человек, оснащенное необходимыми техническими средствами, в котором совершается трудовая деятельность. Организацией рабочего места называется система мероприятий по оснащению рабочего места средствами и предметами труда и их размещению в определенном порядке. 

В соответствии с требованиями эргономики, рабочее место должно быть приспособлено для конкретного вида деятельности и для работников определенной квалификации с учетом их физической и психических возможностей и особенностей. Конструкция рабочего места должна обеспечивать быстроту, безопасность, простоту и экономичность технического обслуживания в нормальных и аварийных условиях; полностью отвечать функциональным требованиям и предполагаемым условиям эксплуатации. При конструировании производственного оборудования необходимо предусматривать возможность регулирования отдельных его элементов с тем, чтобы обеспечивать оптимальное положение работающего. При организации рабочего места учитываются также антропометрические данные пользователя.

Схемы размещения рабочих мест с ВДТ и ПЭВМ должны учитывать расстояния между рабочими столами с видеомониторами (в направлении тыла поверхности одного видеомонитора и экрана другого видеомонитора), которое должно быть не менее 2,0 м, а расстояние между боковыми поверхностями видеомониторов - не менее 1,2 м. 

Рабочие места с ВДТ и ПЭВМ в залах электронно-вычислительных машин или в помещениях с источниками вредных производственных факторов должны размещаться в изолированных кабинах с организованным воздухообменом. Рабочие места с ВДТ и ПЭВМ при выполнении творческой работы, требующей значительного умственного напряжения или высокой концентрации внимания, следует изолировать друг от друга перегородками высотой 1,5 - 2,0 м. 

Оконные проемы в помещениях использования ВДТ и ПЭВМ должны быть оборудованы регулируемыми устройствами типа жалюзи, занавесей и др.

Шкафы, сейфы, стеллажи для хранения дисков, дискет, комплектующих деталей, запасных блоков ВДТ и ПЭВМ, инструментов следует располагать в подсобных помещениях, для учебных заведений - в лаборантских. При отсутствии подсобных помещений или лаборантских допускается размещение шкафов, сейфов и стеллажей в помещениях непосредственного использования ВДТ и ПЭВМ при соблюдении требований к площади помещений и требований.

Конструкция рабочего стола должна обеспечивать оптимальное размещение на рабочей поверхности используемого оборудования с учетом его количества и конструктивных особенностей (размер ВДТ и ПЭВМ, клавиатуры, пюпитра и др.). При этом допускается использование рабочих столов различных конструкций, отвечающих современным требованиям эргономики. Покрытие стола должно быть матовым (с коэффициентом отражения 20 – 50\%) и легко чиститься; углы и передняя грань столешницы должны быть закругленными. Параметры стола указаны в Табл. $\ref{tabular:stol}$.

\begin{table} [H]
\label{tabular:stol}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
& \multicolumn{2}{c|}{Высота стола над полом (мм)} \\
\cline{2-3}
\raisebox{1.5ex}[0cm][0cm]{Рост пользователя в обуви (см)}
& Поверхность стола & Пространство для ног, не менее \\
\hline
116 -- 130 & 520 & 400 \\
\hline
131 -- 145 & 580 & 520 \\
\hline
146 -- 160 & 640 & 580 \\
\hline
161 -- 175 & 700 & 640 \\
\hline
\end{tabular}
\end{center}
\caption{Высота рабочей поверхности стола для пользователей}
\end{table}

Рабочий стол должен иметь пространство для ног высотой не менее той, которая указана в Табл. 13., шириной - не менее 500 мм, глубиной на уровне колен - не менее 450 мм и на уровне вытянутых ног - не менее 650 мм. 

Клавиатуру следует располагать на поверхности стола на расстоянии 100 - 300 мм от края, обращенного к пользователю, или на специальной регулируемой по высоте рабочей поверхности, отделенной от основной столешницы.

Конструкция рабочего стула (кресла) должна обеспечивать поддержание рациональной рабочей позы при работе на ВДТ и ПЭВМ, позволять изменять позу с целью снижения статического напряжения мышц шейно-плечевой области и спины для предупреждения развития утомления. Тип рабочего кресла должен выбираться в зависимости от характера и продолжительности работы  с учетом роста пользователя (Табл. $\ref{tabular:stul}$). При длительной работе кресло должно быть массивным, при кратковременной – легкой конструкции, свободно отодвигающееся.

Рабочее кресло должен быть подъемно - поворотным и регулируемым по высоте и углам наклона сиденья и спинки, а также расстоянию спинки от переднего края сиденья, при этом регулировка каждого параметра должна быть независимой, легко осуществляемой и иметь надежную фиксацию.

\begin{table} [H]
\label{tabular:stul}
\begin{center}
\begin{tabular}{|p{0.2\linewidth}|c|c|c|c|c|}
\hline
& \multicolumn{5}{c|}{Рост пользователя в обуви (см)} \\
\cline{2-6}
\raisebox{1.5ex}[0cm][0cm]{Параметры стула}
& 116 -- 130 & 131 -- 145 &  146 -- 160 & 161 -- 175  & Более 175\\
\hline
Высота сиденья над полом (мм) & 300 & 340 & 380 & 420 & 460 \\
\hline
Глубина сиденья	& 270 & 290	& 320 & 340 & 360 \\
\hline
Ширина сиденья, не менее (мм) & 290	& 330 & 360 & 380 & 400 \\
\hline
Высота нижнего края спинки над сиденьем (мм) & 130 & 150 & 160 & 170 & 190 \\
\hline
Высота линии прогиба спинки, не менее (мм) & 280 & 310 & 330 & 360 & 400 \\
\hline
Высота линии прогиба спинки, не менее (мм) & 170 & 190 & 200 & 210 & 220 \\
\hline
Радиус изгиба переднего края сиденья (мм) & \multicolumn{5}{c|}{20 -- 50} \\
\hline
Угол наклона сиденья (град)	& \multicolumn{5}{c|}{0 -- 4} \\
\hline
Угол наклона спинки (град)	& \multicolumn{5}{c|}{95 -- 108} \\
\hline
Радиус спинки в пален, не менее (мм) & \multicolumn{5}{c|}{300} \\
\hline
\end{tabular}
\end{center}
\caption{Параметры рабочего кресла}
\end{table}

Поверхность сиденья, спинки и других элементов кресла должна быть полумягкой, с нескользящим, неэлектризующимся и воздухопроницаемым покрытием, обеспечивающим легкую очистку от загрязнений. 

Визуальные эргономические параметры ВДТ являются параметрами безопасности, и их неправильный выбор приводит к ухудшению здоровья пользователей. Все ВДТ должны иметь гигиенический сертификат, включающий, в том числе, оценку визуальных параметров. 

Конструкция ВДТ, его дизайн и совокупность эргономических параметров должны обеспечивать надежное и комфортное считывание отображаемой информации в условиях эксплуатации. Устройство ВДТ должна обеспечивать возможность фронтального наблюдения экрана путем поворота корпуса в горизонтальной плоскости вокруг вертикальной оси в пределах  30 градусов и в вертикальной плоскости вокруг горизонтальной оси в пределах   30 градусов с фиксацией в заданном положении. Дизайн ВДТ должен предусматривать окраску корпуса в спокойные мягкие тона с диффузным рассеиванием света. Корпус ВДТ и ПЭВМ, клавиатура и другие блоки и устройства ПЭВМ должны иметь матовую поверхность одного цвета с коэффициентом отражения 0,4 – 0,6 и не иметь блестящих деталей, способных создавать блики. На лицевой стороне корпуса ВДТ не рекомендуется располагать органы управления, маркировку, какие-либо вспомогательные надписи и обозначения. При необходимости расположения органов управления на лицевой панели они должны закрываться крышкой или быть утоплены в корпусе.

Для обеспечения надежного считывания информации при соответствующей степени комфортности ее восприятия должны быть определены оптимальные и допустимые диапазоны визуальных эргономических параметров. Визуальные эргономические параметры ВДТ и пределы их изменений, в которых должны быть установлены оптимальные и допустимые диапазоны значений, приведены в Табл. $\ref{tabular:visual}$.

\begin{table} [H]
\label{tabular:visual}
\begin{center}
\begin{tabular}{|p{0.3\linewidth}|c|c|}
\hline
& \multicolumn{2}{c|}{Пределы значений параметров} \\
\cline{2-3}
\raisebox{1.5ex}[0cm][0cm]{Наименование параметров}
& Минимум (не менее) & Максимум (не более) \\
\hline
Яркость знака (яркость фона), измеренная в темноте (кд/м2)	& 35 & 120 \\
\hline
Внешняя освещенность экрана (лк) & 100 & 250 \\
\hline
Уголовой размер знака (угл. мин.) & 16 & 60 \\
\hline
\end{tabular}
\end{center}
\caption{Допустимые визуальные эргономические параметры}
\end{table}

Примечание к Табл. $\ref{tabular:visual}$: угловой размер знака – угол между линиями, соединяющими крайние точки знака по высоте и глаз наблюдателя. Угловой размер знака определяется по формуле:  , где h – высота знака, l – расстояние от знака до глаза наблюдения.

При проектировании и разработке ВДТ сочетания визуальных эргономических параметров и их значения, соответствующие оптимальным и допустимым диапазонам, полученные в результате испытаний в специализированных лабораториях, аккредитованных в установленном порядке, и подтвержденные соответствующими протоколами, должны быть внесены в техническую документацию на ВДТ. При отсутствии в технической документации на ВДТ данных об оптимальных и допустимых диапазонах значений эргономических параметров эксплуатации ВДТ не допускается. 

Конструкция ВДТ должна предусматривать наличие ручек регулировки яркости и контраста, обеспечивающих возможность регулировки этих параметров от минимальных до максимальных значений. 

В целях обеспечения защиты от электромагнитных и электростатических полей допускается применение экранных фильтров, специальных экранов и других средств индивидуальной защиты, прошедших испытания в аккредитованных лабораториях и имеющих соответствующий гигиенический сертификат. Допустимые значения параметров излучений указаны в Табл. $\ref{tabular:electro}$.

Конструкция ВДТ и ПЭВМ должна обеспечивать мощность экспозиционной дозы рентгеновского излучения в любой точке на расстоянии 0,05 м от экрана и корпуса ВДТ, при любых положениях регулировочных устройств не должна превышать100 мкР/ч.


\begin{table} [H]
\label{tabular:micro1}
\begin{center}
\begin{tabular}{|p{0.6\linewidth}|c||}
\hline
Наименование параметров & Допустимое значение \\
\hline
Напряженность электромагнитного поля по электрической составляющей на расстоянии 50 см от видеомонитора	& 10 В/м \\
\hline
Напряженность электромагнитного поля по электрической составляющей на расстоянии 50 см от поверхности видеомонитора	& 0,3 А/м \\
\hline
Напряженность электростатического поля не должна превышать:	& \\
\hline
	Для взрослых пользователей	& 20 кВ/м \\
\hline
	Для детей дошкольных учреждений и учащихся средних специальных и высших учебных заведений & 15 кВ/м \\
\hline
Напряженность электромагнитного поля на расстоянии 40 см вокруг ВДТ по электрической составляющей должна быть не более:	& \\
\hline
	В диапазоне частот 5 Гц – 2 кГц	& 25 В/м \\
\hline
	В диапазоне частот 2 кГц – 400 кГц	& 2,5 В/м \\
\hline
Плотность магнитного потока должна быть не более: & \\
\hline
	В диапазоне частот 5 Гц – 2 кГц	& 250 нТл \\
\hline
	В диапазоне частот 2 кГц – 400 кГц	& 25 нТл \\
\hline
Поверхностный электростатический потенциал не должен превышать & 500 В \\
\hline
\end{tabular}
\end{center}
\caption{Допустимые значения параметров неионизирующих электромагнитных излучений}
\end{table}

Конструкция клавиатуры должна предусматривать: 
\begin{itemize}
\item исполнение в виде отдельного устройства с возможностью свободного перемещения
\item опорное приспособление, позволяющее изменять угол наклона поверхности клавиатуры в пределах от 5 до 15 градусов
\item высоту среднего ряда клавиш не более 30 мм
\item расположение часто используемых клавиш в центре, внизу и справа, редко используемых - вверху и слева
\item выделение цветом, размером, формой и местом расположения функциональных групп клавиш
\item минимальный размер клавиш - 13 мм, оптимальный - 15 мм
\item клавиши с углублением в центре и шагом 19  1 мм
\item расстояние между клавишами не менее 3 мм
\item одинаковый ход для всех клавиш с минимальным сопротивлением нажатию 0,25 Н и максимальным - не более 1,5 Н
\item звуковую обратную связь от включения клавиш с регулировкой уровня звукового сигнала и возможности ее отключения
\item 
\item 
\end{itemize}

При работе за ПЭВМ очень важно сохранять правильную осанку, при которой позвоночник будет отдыхать, а не напрягаться. В этом помогает хорошо подобранное рабочее кресло. Спинка кресла должна поддерживать нижнюю половину спины, но при этом не быть жестко закрепленной, чтобы не препятствовать движениям в процессе работы. Ноги должны большую часть времени стоять на полу полной ступней, согнуты чуть больше, чем под прямым углом. 

Голова должна быть немного наклонена вперед, это наиболее естественное состояние. Монитор необходимо установит так, чтобы расстояние от глаз пользователя до любой точки монитора было примерно одинаковое и составляло 50 – 70 см. 

При работе на клавиатуре руки не должны находиться на весу, а опираться на подлокотники. Клавиатура обязательно должна располагаться чуть ниже локтя. Угол, образуемый между плечом и предплечьем, должен составлять около 120 градусов. 

\section{Освещение}
Помещения с ВДТ и ПЭВМ должны иметь естественное и искусственное освещение. Естественное освещение должно осуществляться через светопроемы, ориентированные преимущественно на север и северо-восток. Рабочие места с видеотерминалами (ВДТ) и персональными электронно-вычислительными машинами (ПЭВМ) по отношению к световым проемам должны располагаться так, чтобы естественный свет падал сбоку, преимущественно слева.

Искусственное освещение в помещениях эксплуатации ВДТ и ПЭВМ должно осуществляться системой общего равномерного освещения. В производственных и административно - общественных помещениях, в случаях преимущественной работы с документами, допускается применение системы комбинированного освещения.

Освещенность на поверхности стола в зоне размещения рабочего документа должна быть 300 - 500 лк. Допускается установка светильников местного освещения для подсветки документов. Местное освещение не должно создавать бликов на поверхности экрана и увеличивать освещенность экрана более 300 лк. 

В качестве источников света при искусственном освещении должны применяться преимущественно люминесцентные лампы, так как у них высокая светоотдача (до 120лм/Вт и более), продолжительный срок службы (до 10 000ч.), малая яркость светящейся поверхности, близкий к естественному спектральный состав излучаемого света, что обеспечивает хорошую светопередачу. Допускается применение ламп накаливания в светильниках местного освещения. Светильники местного освещения должны иметь непросвечивающий отражатель с защитным углом не менее 40 градусов.

Общее освещение следует выполнять в виде сплошных или прерывистых линий светильников, расположенных сбоку от рабочих мест, параллельно линии зрения пользователя при рядном расположении ВДТ и ПЭВМ. При периметральном расположении компьютеров линии светильников должны располагаться локализовано над рабочим столом ближе к его переднему краю, обращенному к оператору. 

Для обеспечения нормируемых значений освещенности в помещениях использования ВДТ и ПЭВМ следует проводить чистку стекол оконных рам и светильников не реже двух раз в год и проводить своевременную замену перегоревших ламп.

\section{Электробезопасность}
Электробезопасность – система организационных и технических мероприятий и средств, обеспечивающих защиту людей от вредного и опасного воздействия электрического тока, электрической дуги, электромагнитного поля и статического электричества. При прохождении через организм, электрический ток оказывает следующие виды воздействий:
\begin{itemize}
\item термическое действие (выражается в ожогах отдельных участков тела, нагреве кровеносных сосудов, нервов и иных тканей)
\item электролитическое воздействие (выражается в разложении крови и других органических жидкостей в организме, что вызывает существенное изменение в их физико-химических составах)
\item биологическое действие (выражается в раздражении и возбуждении живых тканей организма, нарушением внутренних биоэлектрических процессов, протекающих в нормально действующем организме, и тесно связанных с его жизненными функциями)
\end{itemize}

К числу основных причин, вызывающих поражение электрическим током относятся: случайное прикосновение к токоведущим частям, или приближение к ним на опасное расстояние; появление напряжений на металлических не токоведущих частях электрооборудования в результате пробоя или ошибочного включения; появление напряжения на поверхности земли в результате стекания тока в землю.

В соответствии с правилами устройства электроустановок (ПУЭ) для защиты должна применяется хотя бы одна из следующих нормативных мер: малые напряжения; защитное заземление, зануление или отключение. Защитное заземление - это преднамеренное соединение с землёй или её эквивалентом металлических нетоковедущих частей электроустановок, которые могут оказаться под напряжением. Основной смысл данной меры защиты – создание параллельного пути с наименьшим сопротивлением, при контакте человека с корпусом во время пробоя. Недостатком защитного заземления является постоянное наличие напряжения на корпусе. Защитное заземление должно присутствовать во всех помещениях. Зануление - это преднамеренное соединение с нулевым защитным проводником. Так как возникают большие токи, то необходимы плавкие вставки, но вследствие медленности срабатывания (5-7 сек), они предназначены в основном для противопожарной защиты. Отключение – это использование специального устройства, приводящее в действие отключающий механизм за время меньшее 0.1 сек. 

Признаки электрической опасности помещения:
\begin{itemize}
\item Повышенная опасность: наличие токопроводящего пола, сырости (> 75\%) или токопроводящей пыли, повышенная температура воздуха (> $30^\circ$C), возможность одновременного прикосновения к металлическим частям электроустановок и заземлённым конструкциям
\item Особая опасность: особая влажность (~100\%, поверхности покрыты влагой), химически активная среда, могущая разрушать изоляцию, наличие электроустановок, эксплуатируемых на открытом воздухе
\end{itemize}
В зависимости от признаков электрической опасности помещения подразделяются на 3 класса: помещения без повышенной опасности (без признаков опасности), помещения повышенной опасности (если присутствует хотя бы один признак повышенной опасности), особо опасные помещения (присутствует хотя бы один признак особой опасности, или более одного признака повышенной опасности).

Заземлению и занулению подлежат все электроустановки во взрывоопасных помещениях, в помещениях повышенной и особой опасности при напряжениях свыше 36В и в других помещениях при напряжениях свыше 500В.

\section{Пожаробезопасность}
Согласно ГОСТ 12.1.004-91 существуют следующие опасные факторы:
\begin{itemize}
\item пламя и искры
\item повышенная температура окружающей среды
\item токсичные продукты горения и термического разложения
\item пониженная концентрация кислорода
\end{itemize}
Противопожарная защита обеспечивается следующими мерами:
\begin{itemize}
\item применение средств пожаротушения, установка сигнализации и устройств тушения, ограничивающих распространение пожара, мероприятия по эвакуации людей, нали-чие средств индивидуальной защиты и средств противодымной защиты
\item наличие противопожарных перегородок и отсеков, устройств автоматического от-ключения систем
\item планировка эвакуационных путей и выходов
\item оповещение людей
\item технические средства для эвакуации и спасения людей
\item наличие огнетушащих веществ
\end{itemize}

Пожарную опасность в ВЦ представляют носители информации, поэтому помещение должно быть оборудовано несгораемыми стеллажами и шкафами. Хранение перфокарт, лент, дисков должно производиться в металлических кассетах. Не допускается размещение складских помещений, а также пожаровзрывоопасных производств над и под залами ЭВМ, а также смежных с ними помещениях.

Система вентиляции ВЦ должна быть оборудована устройством, обеспечивающим автоматическое отключение ее при пожаре, а также огнедымозадерживающими устрой-ствами.

Подача воздуха к ЭВМ для охлаждения должна осуществляться по самостоятельному воздуховоду. Присоединение этих воздуховодов к общему коллектору допускается только после огне- и дымозадерживающих клапанов.

Система электропитания ЭВМ должна иметь блокировку, обеспечивающую отключение ЭВМ в случае остановки системы кондиционирования и охлаждения. Промывка ячеек и других съемных устройств горючими жидкостями допускается только в специальных помещениях, оборудованных проточно-вытяжной системой.

В здании ВЦ должна быть предусмотрена автоматическая пожарная сигнализация. В залах ЭВМ, за подвесными потолками, в хранилищах информации, кладовых запасного оборудования необходимо устанавливать извещатели, реагирующие на дым. Во всех других помещениях ВЦ допускается установка типовых пожарных извещателей.

Для тушения возможных пожаров ВЦ необходимо оборудовать автоматическими уста-новками объемного газового тушения с выводом огнегасительного вещества в кабельные каналы и потоки.

\chapter{Выводы}
В работе были рассмотрены различные подходы к построению рекомендательных систем. Среди рассмотренных подходов была выделена группа методов, основывающихся на решении задачи классификации для которой и проводились дальнейшие рассуждения (для остальных рассмотренных методов задача имеет очевидное решение). 

Для выявления статистически значимых различий между двумя алгоритмами интерпретации, были применены 2 подхода (TDI, A/B), обычно используемые для оценки качества ранжирования поисковых систем.

Был предложен наивный алгоритм для формирования объяснений рекомендаций, а так же несколько последовательных модификаций. Наивный алгоритм показал малую вариативность, т.к. выбирал принципиально более сильные признаки. Поэтому его практические испытания не проводились.

Для проверки базовых гипотез (о роли признака и значимости объяснения рекомендации) был проведен эксперимент по сравнению алгоритма без интерпретации рекомендаций и алгоритма, использующего гипотезу о нормальности. По результатам эксперимента, обе гипотезы подтвердились, что дало основания для дальнейших модификаций алгоритма интерпретации.

Был проведен эксперимент по сравнению алгоритма, использующего гипотезу о нормальности распределения вклада каждого признака и модифицированного алгоритма, представляющего распределение вклада каждого признака в виде смеси гауссиан. В результате эксперимента удалось выявить статистически значимое превосходство последнего алгоритма над первым по числу кликов на рекомендацию. 
[сюда еще эксперимент normal vs truncated]

В работе были предлдожены конкретные алгоримы интерпретации рекомендаций для рекомендательных систем, использующих Factorization Machines или ансамблей забывчивых деревьев. Однако, что бы применить предложенные методы для другогой рекомендательной системы, необходимо только предоставить алгоритм определения абсолютного вклада признака в вероятность, остальные шаги останутся без изменений.

\end{document}
